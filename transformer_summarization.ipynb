{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=32000, min_freq=2):\n",
    "        \"\"\"\n",
    "        Initialize simple tokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Maximum vocabulary size\n",
    "            min_freq (int): Minimum frequency for a token to be included\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.bos_token = \"<bos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "\n",
    "        # Add special tokens to vocabulary\n",
    "        self.word2idx = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3,\n",
    "        }\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing special characters\n",
    "        3. Splitting into words\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "\n",
    "        return words\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            word_freq.update(words)\n",
    "\n",
    "        # Sort words by frequency\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        # Add most frequent words to vocabulary\n",
    "        for word, freq in sorted_words:\n",
    "            if len(self.word2idx) >= self.vocab_size:\n",
    "                break\n",
    "            if freq >= self.min_freq:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Encode text to token indices.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            add_special_tokens (bool): Whether to add BOS/EOS tokens\n",
    "        \"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "\n",
    "        # Convert words to indices\n",
    "        indices = []\n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx[self.bos_token])\n",
    "\n",
    "        for word in words:\n",
    "            idx = self.word2idx.get(word, self.word2idx[self.unk_token])\n",
    "            indices.append(idx)\n",
    "\n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx[self.eos_token])\n",
    "\n",
    "        return np.array(indices)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"\n",
    "        Decode token indices to text.\n",
    "\n",
    "        Args:\n",
    "            indices: Array of token indices\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                word = self.idx2word[idx]\n",
    "                if word in [self.pad_token, self.bos_token, self.eos_token]:\n",
    "                    continue\n",
    "                words.append(word)\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        \"\"\"\n",
    "        Pad sequence to max_length.\n",
    "\n",
    "        Args:\n",
    "            sequence: Array of token indices\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        if len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length]\n",
    "        else:\n",
    "            padding = [self.word2idx[self.pad_token]] * (max_length - len(sequence))\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "\n",
    "        return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSumDataset:\n",
    "    def __init__(self, max_seq_length=512, batch_size=32, vocab_size=32000):\n",
    "        \"\"\"\n",
    "        Initialize XLSum dataset.\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            batch_size (int): Batch size for training\n",
    "            vocab_size (int): Vocabulary size for tokenizer\n",
    "        \"\"\"\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(\"csebuetnlp/xlsum\", \"indonesian\", cache_dir=\"cache\")\n",
    "\n",
    "        # Build vocabulary\n",
    "        self._build_vocab()\n",
    "\n",
    "        # Prepare data\n",
    "        self.train_data = self._prepare_data(\"train\")\n",
    "        self.validation_data = self._prepare_data(\"validation\")\n",
    "        self.test_data = self._prepare_data(\"test\")\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Build vocabulary from training data.\"\"\"\n",
    "        texts = []\n",
    "        for item in self.dataset[\"train\"]:\n",
    "            texts.append(item[\"text\"])\n",
    "            texts.append(item[\"summary\"])\n",
    "\n",
    "        self.tokenizer.build_vocab(texts)\n",
    "        print(f\"Vocabulary size: {len(self.tokenizer.word2idx)}\")\n",
    "\n",
    "    def _prepare_data(self, split):\n",
    "        \"\"\"\n",
    "        Prepare data for a specific split.\n",
    "\n",
    "        Args:\n",
    "            split (str): Dataset split (\"train\", \"validation\", or \"test\")\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for item in self.dataset[split]:\n",
    "            # Tokenize text and summary\n",
    "            text_tokens = self.tokenizer.encode(item[\"text\"])\n",
    "            summary_tokens = self.tokenizer.encode(item[\"summary\"])\n",
    "\n",
    "            # Pad sequences\n",
    "            text_tokens = self.tokenizer.pad_sequence(text_tokens, self.max_seq_length)\n",
    "            summary_tokens = self.tokenizer.pad_sequence(\n",
    "                summary_tokens, self.max_seq_length\n",
    "            )\n",
    "\n",
    "            data.append({\"text\": text_tokens, \"summary\": summary_tokens})\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_batch(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Get a batch of data.\n",
    "\n",
    "        Args:\n",
    "            split (str): Dataset split (\"train\", \"validation\", or \"test\")\n",
    "        \"\"\"\n",
    "        # Map split name to data attribute\n",
    "        split_map = {\n",
    "            \"train\": \"train_data\",\n",
    "            \"validation\": \"validation_data\",\n",
    "            \"test\": \"test_data\",\n",
    "        }\n",
    "\n",
    "        if split not in split_map:\n",
    "            raise ValueError(\n",
    "                f\"Invalid split: {split}. Must be one of {list(split_map.keys())}\"\n",
    "            )\n",
    "\n",
    "        data = getattr(self, split_map[split])\n",
    "\n",
    "        # Randomly sample batch_size examples\n",
    "        indices = np.random.choice(len(data), self.batch_size, replace=False)\n",
    "        batch = [data[i] for i in indices]\n",
    "\n",
    "        # Stack text and summary tensors, ensuring integer type\n",
    "        text_batch = np.stack([item[\"text\"] for item in batch]).astype(np.int64)\n",
    "        summary_batch = np.stack([item[\"summary\"] for item in batch]).astype(np.int64)\n",
    "\n",
    "        return text_batch, summary_batch\n",
    "\n",
    "    def decode_batch(self, indices_batch):\n",
    "        \"\"\"\n",
    "        Decode a batch of token indices to text.\n",
    "\n",
    "        Args:\n",
    "            indices_batch: Batch of token indices\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for indices in indices_batch:\n",
    "            text = self.tokenizer.decode(indices)\n",
    "            texts.append(text)\n",
    "        return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Multi-Head Attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W_q = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_k = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_v = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_o = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "\n",
    "        # Initialize biases\n",
    "        self.b_q = np.zeros(d_model)\n",
    "        self.b_k = np.zeros(d_model)\n",
    "        self.b_v = np.zeros(d_model)\n",
    "        self.b_o = np.zeros(d_model)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return np.transpose(x, (0, 2, 1, 3))\n",
    "\n",
    "    def combine_heads(self, x, batch_size):\n",
    "        \"\"\"Combine heads back together.\"\"\"\n",
    "        x = np.transpose(x, (0, 2, 1, 3))\n",
    "        return x.reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            q: Query shape == (..., seq_len_q, d_k)\n",
    "            k: Key shape == (..., seq_len_k, d_k)\n",
    "            v: Value shape == (..., seq_len_v, d_v)\n",
    "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        matmul_qk = np.matmul(q, np.transpose(k, (0, 1, 3, 2)))\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"matmul_qk\"] = matmul_qk\n",
    "\n",
    "        # Scale matmul_qk\n",
    "        dk = np.sqrt(self.d_k)\n",
    "        scaled_attention_logits = matmul_qk / dk\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"scaled_attention_logits\"] = scaled_attention_logits\n",
    "\n",
    "        # Add mask if provided\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "            self.cache[\"mask\"] = mask\n",
    "\n",
    "        # Softmax is normalized on the last axis (seq_len_k)\n",
    "        attention_weights = self.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"attention_weights\"] = attention_weights\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(\n",
    "                1, 1 - self.dropout, size=attention_weights.shape\n",
    "            ) / (1 - self.dropout)\n",
    "            attention_weights = attention_weights * dropout_mask\n",
    "            self.cache[\"dropout_mask\"] = dropout_mask\n",
    "\n",
    "        output = np.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            q: Query input\n",
    "            k: Key input\n",
    "            v: Value input\n",
    "            mask: Optional mask for attention\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.cache[\"q\"] = q\n",
    "        self.cache[\"k\"] = k\n",
    "        self.cache[\"v\"] = v\n",
    "\n",
    "        # Linear projections and split into heads\n",
    "        q = np.matmul(q, self.W_q) + self.b_q\n",
    "        k = np.matmul(k, self.W_k) + self.b_k\n",
    "        v = np.matmul(v, self.W_v) + self.b_v\n",
    "\n",
    "        # Store projections for backward pass\n",
    "        self.cache[\"q_proj\"] = q\n",
    "        self.cache[\"k_proj\"] = k\n",
    "        self.cache[\"v_proj\"] = v\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Store split heads for backward pass\n",
    "        self.cache[\"q_heads\"] = q\n",
    "        self.cache[\"k_heads\"] = k\n",
    "        self.cache[\"v_heads\"] = v\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "\n",
    "        # Combine heads\n",
    "        concat_attention = self.combine_heads(scaled_attention, batch_size)\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"concat_attention\"] = concat_attention\n",
    "\n",
    "        # Final linear projection\n",
    "        output = np.matmul(concat_attention, self.W_o) + self.b_o\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get the expected shapes from cached values\n",
    "        expected_batch_size = self.cache[\"q\"].shape[0]\n",
    "        expected_seq_len = self.cache[\"q\"].shape[1]\n",
    "        expected_d_model = self.cache[\"q\"].shape[2]\n",
    "\n",
    "        # Ensure dout has the correct shape and dimensions\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has an extra dimension, reshape it\n",
    "            dout = dout.reshape(-1, dout.shape[-2], dout.shape[-1])\n",
    "\n",
    "        # Handle batch size mismatch\n",
    "        if dout.shape[0] != expected_batch_size:\n",
    "            # If batch size doesn't match, reshape to expected batch size\n",
    "            total_samples = dout.shape[0]\n",
    "            if total_samples % expected_batch_size == 0:\n",
    "                # Reshape by combining multiple samples into the expected batch size\n",
    "                dout = dout.reshape(expected_batch_size, -1, dout.shape[-1])\n",
    "                # Average the gradients across the combined samples\n",
    "                dout = dout.mean(axis=1, keepdims=True).repeat(\n",
    "                    dout.shape[1] // expected_batch_size, axis=1\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot reshape dout from shape {dout.shape} to batch size {expected_batch_size}\"\n",
    "                )\n",
    "\n",
    "        # Handle sequence length mismatch\n",
    "        if dout.shape[1] != expected_seq_len:\n",
    "            # If sequence length doesn't match, we need to handle it\n",
    "            if dout.shape[1] < expected_seq_len:\n",
    "                # If sequence is shorter, pad with zeros\n",
    "                padding = np.zeros(\n",
    "                    (dout.shape[0], expected_seq_len - dout.shape[1], dout.shape[2])\n",
    "                )\n",
    "                dout = np.concatenate([dout, padding], axis=1)\n",
    "            else:\n",
    "                # If sequence is longer, truncate\n",
    "                dout = dout[:, :expected_seq_len, :]\n",
    "\n",
    "        # Handle model dimension mismatch\n",
    "        if dout.shape[2] != expected_d_model:\n",
    "            raise ValueError(\n",
    "                f\"Model dimension mismatch: got {dout.shape[2]}, expected {expected_d_model}\"\n",
    "            )\n",
    "\n",
    "        batch_size = dout.shape[0]\n",
    "        seq_len_q = dout.shape[1]  # Get sequence length from input\n",
    "\n",
    "        # Get cached values\n",
    "        q = self.cache[\"q\"]\n",
    "        k = self.cache[\"k\"]\n",
    "        v = self.cache[\"v\"]\n",
    "        q_proj = self.cache[\"q_proj\"]\n",
    "        k_proj = self.cache[\"k_proj\"]\n",
    "        v_proj = self.cache[\"v_proj\"]\n",
    "        q_heads = self.cache[\"q_heads\"]  # (batch, heads, seq_len_q, d_k)\n",
    "        k_heads = self.cache[\"k_heads\"]  # (batch, heads, seq_len_k, d_k)\n",
    "        v_heads = self.cache[\"v_heads\"]  # (batch, heads, seq_len_v, d_k)\n",
    "        concat_attention = self.cache[\"concat_attention\"]\n",
    "        attention_weights = self.cache[\n",
    "            \"attention_weights\"\n",
    "        ]  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Get sequence lengths from cached tensors\n",
    "        seq_len_k = attention_weights.shape[\n",
    "            -1\n",
    "        ]  # Get key sequence length from attention weights\n",
    "        seq_len_v = v_heads.shape[-2]  # Get value sequence length from v_heads\n",
    "\n",
    "        # Verify all dimensions match\n",
    "        assert dout.shape == (\n",
    "            expected_batch_size,\n",
    "            expected_seq_len,\n",
    "            expected_d_model,\n",
    "        ), f\"Shape mismatch: got {dout.shape}, expected {(expected_batch_size, expected_seq_len, expected_d_model)}\"\n",
    "        assert concat_attention.shape == (\n",
    "            expected_batch_size,\n",
    "            expected_seq_len,\n",
    "            expected_d_model,\n",
    "        ), f\"concat_attention shape mismatch: got {concat_attention.shape}, expected {(expected_batch_size, expected_seq_len, expected_d_model)}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. output projection\n",
    "        dW_o = np.matmul(\n",
    "            concat_attention.transpose(0, 2, 1), dout\n",
    "        )  # (d_model, d_model)\n",
    "        db_o = np.sum(dout, axis=(0, 1))  # (d_model,)\n",
    "        dconcat = np.matmul(dout, self.W_o.T)  # (batch, seq_len_q, d_model)\n",
    "\n",
    "        # Verify dconcat shape\n",
    "        assert dconcat.shape == (\n",
    "            batch_size,\n",
    "            seq_len_q,\n",
    "            self.d_model,\n",
    "        ), f\"dconcat shape mismatch: got {dconcat.shape}, expected {(batch_size, seq_len_q, self.d_model)}\"\n",
    "\n",
    "        # Gradient through head combination\n",
    "        # Ensure dconcat is properly reshaped\n",
    "        dscaled_attention = dconcat.reshape(\n",
    "            batch_size, -1, self.num_heads, self.d_k\n",
    "        )  # (batch, seq_len_q, heads, d_k)\n",
    "        dscaled_attention = np.transpose(\n",
    "            dscaled_attention, (0, 2, 1, 3)\n",
    "        )  # (batch, heads, seq_len_q, d_k)\n",
    "\n",
    "        # Verify dscaled_attention shape\n",
    "        assert dscaled_attention.shape == (\n",
    "            batch_size,\n",
    "            self.num_heads,\n",
    "            seq_len_q,\n",
    "            self.d_k,\n",
    "        ), f\"dscaled_attention shape mismatch: got {dscaled_attention.shape}, expected {(batch_size, self.num_heads, seq_len_q, self.d_k)}\"\n",
    "\n",
    "        # Reshape tensors to combine batch and head dimensions\n",
    "        attention_weights_reshaped = attention_weights.reshape(\n",
    "            -1, seq_len_q, seq_len_k\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "        dscaled_attention_reshaped = dscaled_attention.reshape(\n",
    "            -1, seq_len_q, self.d_k\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "        v_heads_reshaped = v_heads.reshape(\n",
    "            -1, seq_len_v, self.d_k\n",
    "        )  # (batch*heads, seq_len_v, d_k)\n",
    "\n",
    "        # Verify the number of heads matches\n",
    "        num_heads = batch_size * self.num_heads\n",
    "        assert (\n",
    "            attention_weights_reshaped.shape[0]\n",
    "            == dscaled_attention_reshaped.shape[0]\n",
    "            == v_heads_reshaped.shape[0]\n",
    "            == num_heads\n",
    "        ), (\n",
    "            f\"Number of heads mismatch: attention_weights={attention_weights_reshaped.shape[0]}, \"\n",
    "            f\"dscaled_attention={dscaled_attention_reshaped.shape[0]}, v_heads={v_heads_reshaped.shape[0]}, \"\n",
    "            f\"expected={num_heads}\"\n",
    "        )\n",
    "\n",
    "        # Compute gradients through attention weights\n",
    "        dv_heads_reshaped = np.zeros_like(\n",
    "            v_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_v, d_k)\n",
    "        dattention_weights_reshaped = np.zeros_like(\n",
    "            attention_weights_reshaped\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Process each head separately to avoid broadcasting issues\n",
    "        for i in range(num_heads):\n",
    "            # For v_heads gradient:\n",
    "            # attention_weights[i].T: (seq_len_k, seq_len_q)\n",
    "            # dscaled_attention[i]: (seq_len_q, d_k)\n",
    "            # Result should be: (seq_len_k, d_k)\n",
    "            # But we need (seq_len_v, d_k) for v_heads\n",
    "\n",
    "            # First compute the gradient through attention weights\n",
    "            dattention_weights_reshaped[i] = np.matmul(\n",
    "                dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                v_heads_reshaped[i].T,  # (d_k, seq_len_v)\n",
    "            )  # Result: (seq_len_q, seq_len_v)\n",
    "\n",
    "            # Then compute the gradient for v_heads\n",
    "            # We need to ensure the sequence lengths match\n",
    "            if seq_len_k == seq_len_v:\n",
    "                # If sequence lengths match, we can directly compute\n",
    "                dv_heads_reshaped[i] = np.matmul(\n",
    "                    attention_weights_reshaped[i].T,  # (seq_len_k, seq_len_q)\n",
    "                    dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                )  # Result: (seq_len_k, d_k)\n",
    "            else:\n",
    "                # If sequence lengths don't match, we need to handle it differently\n",
    "                # For now, we'll use a simple approach: take the first min(seq_len_k, seq_len_v) positions\n",
    "                min_len = min(seq_len_k, seq_len_v)\n",
    "                dv_heads_reshaped[i, :min_len] = np.matmul(\n",
    "                    attention_weights_reshaped[\n",
    "                        i, :, :min_len\n",
    "                    ].T,  # (min_len, seq_len_q)\n",
    "                    dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                )  # Result: (min_len, d_k)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        dv_heads = dv_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_v, self.d_k\n",
    "        )\n",
    "        dattention_weights = dattention_weights_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_q, seq_len_v\n",
    "        )\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if self.dropout > 0 and \"dropout_mask\" in self.cache:\n",
    "            dattention_weights *= self.cache[\"dropout_mask\"]\n",
    "\n",
    "        # Gradient through softmax\n",
    "        dscaled_logits = (\n",
    "            dattention_weights * attention_weights * (1 - attention_weights)\n",
    "        )  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Gradient through scaling\n",
    "        dmatmul_qk = dscaled_logits / np.sqrt(\n",
    "            self.d_k\n",
    "        )  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Gradient through QK multiplication\n",
    "        # Reshape for batch matrix multiplication\n",
    "        dmatmul_qk_reshaped = dmatmul_qk.reshape(\n",
    "            -1, seq_len_q, seq_len_k\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "        k_heads_reshaped = k_heads.reshape(\n",
    "            -1, seq_len_k, self.d_k\n",
    "        )  # (batch*heads, seq_len_k, d_k)\n",
    "        q_heads_reshaped = q_heads.reshape(\n",
    "            -1, seq_len_q, self.d_k\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "\n",
    "        # Compute gradients for each head separately\n",
    "        dq_heads_reshaped = np.zeros_like(\n",
    "            q_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "        dk_heads_reshaped = np.zeros_like(\n",
    "            k_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_k, d_k)\n",
    "\n",
    "        for i in range(num_heads):\n",
    "            # Compute gradients for q_heads\n",
    "            dq_heads_reshaped[i] = np.matmul(\n",
    "                dmatmul_qk_reshaped[i],  # (seq_len_q, seq_len_k)\n",
    "                k_heads_reshaped[i],  # (seq_len_k, d_k)\n",
    "            )  # Result: (seq_len_q, d_k)\n",
    "\n",
    "            # Compute gradients for k_heads\n",
    "            dk_heads_reshaped[i] = np.matmul(\n",
    "                dmatmul_qk_reshaped[i].T,  # (seq_len_k, seq_len_q)\n",
    "                q_heads_reshaped[i],  # (seq_len_q, d_k)\n",
    "            )  # Result: (seq_len_k, d_k)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        dq_heads = dq_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_q, self.d_k\n",
    "        )\n",
    "        dk_heads = dk_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_k, self.d_k\n",
    "        )\n",
    "\n",
    "        # Gradient through head splitting\n",
    "        dq_proj = self.combine_heads(dq_heads, batch_size)\n",
    "        dk_proj = self.combine_heads(dk_heads, batch_size)\n",
    "        dv_proj = self.combine_heads(dv_heads, batch_size)\n",
    "\n",
    "        # Gradient through linear projections\n",
    "        dW_q = np.matmul(q.transpose(0, 2, 1), dq_proj)\n",
    "        db_q = np.sum(dq_proj, axis=(0, 1))\n",
    "        dq = np.matmul(dq_proj, self.W_q.T)\n",
    "\n",
    "        dW_k = np.matmul(k.transpose(0, 2, 1), dk_proj)\n",
    "        db_k = np.sum(dk_proj, axis=(0, 1))\n",
    "        dk = np.matmul(dk_proj, self.W_k.T)\n",
    "\n",
    "        dW_v = np.matmul(v.transpose(0, 2, 1), dv_proj)\n",
    "        db_v = np.sum(dv_proj, axis=(0, 1))\n",
    "        dv = np.matmul(dv_proj, self.W_v.T)\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dW_q = dW_q\n",
    "        self.db_q = db_q\n",
    "        self.dW_k = dW_k\n",
    "        self.db_k = db_k\n",
    "        self.dW_v = dW_v\n",
    "        self.db_v = db_v\n",
    "        self.dW_o = dW_o\n",
    "        self.db_o = db_o\n",
    "\n",
    "        return dq, dk, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        \"\"\"\n",
    "        Initialize positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        position = np.arange(max_seq_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = np.zeros((max_seq_length, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pe = pe[np.newaxis, :, :]  # Shape: (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, : x.shape[1], :]\n",
    "\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, vocab_size, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize token embeddings and positional encoding.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            d_model (int): Model dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize token embeddings\n",
    "        self.token_embeddings = np.random.normal(0, 0.02, (vocab_size, d_model))\n",
    "\n",
    "        # Initialize positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length) containing token indices\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # Get token embeddings\n",
    "        embeddings = self.token_embeddings[\n",
    "            x\n",
    "        ]  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Store embeddings before scaling for backward pass\n",
    "        self.cache[\"embeddings\"] = embeddings\n",
    "\n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * np.sqrt(self.d_model)\n",
    "\n",
    "        # Store scaled embeddings for backward pass\n",
    "        self.cache[\"scaled_embeddings\"] = embeddings\n",
    "\n",
    "        # Add positional encoding\n",
    "        embeddings = self.positional_encoding.forward(embeddings)\n",
    "\n",
    "        # Store embeddings before dropout for backward pass\n",
    "        self.cache[\"embeddings_before_dropout\"] = embeddings\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(\n",
    "                1, 1 - self.dropout, size=embeddings.shape\n",
    "            ) / (1 - self.dropout)\n",
    "            embeddings = embeddings * dropout_mask\n",
    "            self.cache[\"dropout_mask\"] = dropout_mask\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of embeddings.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]  # Input token indices\n",
    "        dropout_mask = self.cache.get(\"dropout_mask\")\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum the gradients across layers\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == self.d_model\n",
    "        ), f\"Expected d_model dimension to be {self.d_model}, got {dout.shape[2]}\"\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if dropout_mask is not None:\n",
    "            dout = dout * dropout_mask\n",
    "\n",
    "        # Gradient through positional encoding (no parameters to update)\n",
    "        # The positional encoding is deterministic, so we just pass the gradient through\n",
    "\n",
    "        # Gradient through scaling\n",
    "        dout = dout * np.sqrt(self.d_model)\n",
    "\n",
    "        # Gradient through token embeddings\n",
    "        # We need to accumulate gradients for each token in the vocabulary\n",
    "        d_embeddings = np.zeros_like(\n",
    "            self.token_embeddings\n",
    "        )  # Shape: (vocab_size, d_model)\n",
    "\n",
    "        # Get batch dimensions\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # For each unique token in the batch, accumulate its gradient\n",
    "        unique_tokens = np.unique(x)\n",
    "        for token in unique_tokens:\n",
    "            # Create a mask for each position where this token appears\n",
    "            # Shape: (batch_size, seq_length)\n",
    "            token_mask = x == token\n",
    "\n",
    "            # For each position where the token appears, add its gradient\n",
    "            # We need to handle each position separately to avoid broadcasting issues\n",
    "            for b in range(batch_size):\n",
    "                for s in range(seq_length):\n",
    "                    if token_mask[b, s]:\n",
    "                        # Add the gradient for this position\n",
    "                        d_embeddings[token] += dout[b, s]\n",
    "\n",
    "        # Store gradient for parameter update\n",
    "        self.d_token_embeddings = d_embeddings\n",
    "\n",
    "        # Return gradient for input (not used since input is discrete tokens)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize layer normalization.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            eps (float): Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "        self.cache = {}  # Cache for storing intermediate values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # Compute mean and variance\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self.cache[\"mean\"] = mean\n",
    "        self.cache[\"var\"] = var\n",
    "\n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        self.cache[\"x_norm\"] = x_norm\n",
    "\n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of layer normalization.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]\n",
    "        mean = self.cache[\"mean\"]\n",
    "        var = self.cache[\"var\"]\n",
    "        x_norm = self.cache[\"x_norm\"]\n",
    "\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum over the first dimension to get the correct gradient\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == d_model\n",
    "        ), f\"Expected d_model dimension to be {d_model}, got {dout.shape[2]}\"\n",
    "        assert (\n",
    "            dout.shape[:2] == x_norm.shape[:2]\n",
    "        ), f\"Batch and sequence dimensions must match: dout {dout.shape[:2]} != x_norm {x_norm.shape[:2]}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. beta\n",
    "        # Sum over batch and sequence dimensions to get gradient for each feature\n",
    "        dbeta = np.sum(dout, axis=(0, 1))  # Shape: (d_model,)\n",
    "        assert (\n",
    "            dbeta.shape == self.beta.shape\n",
    "        ), f\"dbeta shape {dbeta.shape} != beta shape {self.beta.shape}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. gamma\n",
    "        # Element-wise multiplication and sum over batch and sequence dimensions\n",
    "        # Ensure x_norm has the same shape as dout\n",
    "        x_norm_reshaped = x_norm.reshape(batch_size, seq_len, d_model)\n",
    "        dgamma = np.sum(dout * x_norm_reshaped, axis=(0, 1))  # Shape: (d_model,)\n",
    "        assert (\n",
    "            dgamma.shape == self.gamma.shape\n",
    "        ), f\"dgamma shape {dgamma.shape} != gamma shape {self.gamma.shape}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. normalized input\n",
    "        dx_norm = dout * self.gamma  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Gradient of loss w.r.t. variance\n",
    "        dvar = np.sum(\n",
    "            dx_norm * (x - mean) * -0.5 * (var + self.eps) ** (-1.5),\n",
    "            axis=-1,\n",
    "            keepdims=True,\n",
    "        )  # Shape: (batch_size, seq_len, 1)\n",
    "\n",
    "        # Gradient of loss w.r.t. mean\n",
    "        dmean = (\n",
    "            np.sum(dx_norm * -1 / np.sqrt(var + self.eps), axis=-1, keepdims=True)\n",
    "            + dvar * np.sum(-2 * (x - mean), axis=-1, keepdims=True) / d_model\n",
    "        )  # Shape: (batch_size, seq_len, 1)\n",
    "\n",
    "        # Gradient of loss w.r.t. input\n",
    "        dx = (\n",
    "            dx_norm / np.sqrt(var + self.eps)\n",
    "            + dvar * 2 * (x - mean) / d_model\n",
    "            + dmean / d_model\n",
    "        )  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.normal(0, 0.02, (d_model, d_ff))\n",
    "        self.W2 = np.random.normal(0, 0.02, (d_ff, d_model))\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # First linear layer with ReLU\n",
    "        h = np.matmul(x, self.W1) + self.b1\n",
    "        self.cache[\"h_pre_relu\"] = h\n",
    "        h = self.relu(h)\n",
    "\n",
    "        # Apply dropout\n",
    "        h = self.dropout_layer(h)\n",
    "        self.cache[\"h_dropout\"] = h\n",
    "\n",
    "        # Second linear layer\n",
    "        output = np.matmul(h, self.W2) + self.b2\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]\n",
    "        h_pre_relu = self.cache[\"h_pre_relu\"]\n",
    "        h_dropout = self.cache[\"h_dropout\"]\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum over the first dimension to get the correct gradient\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == self.d_model\n",
    "        ), f\"Expected d_model dimension to be {self.d_model}, got {dout.shape[2]}\"\n",
    "\n",
    "        batch_size, seq_len, _ = dout.shape\n",
    "\n",
    "        # Gradient of loss w.r.t. second linear layer\n",
    "        # Reshape tensors for matrix multiplication\n",
    "        h_dropout_reshaped = h_dropout.reshape(\n",
    "            -1, self.d_ff\n",
    "        )  # (batch_size * seq_len, d_ff)\n",
    "        dout_reshaped = dout.reshape(\n",
    "            -1, self.d_model\n",
    "        )  # (batch_size * seq_len, d_model)\n",
    "\n",
    "        # Compute gradients for W2 and b2\n",
    "        dW2 = np.matmul(h_dropout_reshaped.T, dout_reshaped)  # (d_ff, d_model)\n",
    "        db2 = np.sum(dout_reshaped, axis=0)  # (d_model,)\n",
    "\n",
    "        # Gradient through second linear layer\n",
    "        dh = np.matmul(dout_reshaped, self.W2.T)  # (batch_size * seq_len, d_ff)\n",
    "        dh = dh.reshape(batch_size, seq_len, self.d_ff)  # (batch_size, seq_len, d_ff)\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if self.dropout > 0:\n",
    "            dh = dh * (h_dropout != 0) / (1 - self.dropout)\n",
    "\n",
    "        # Gradient through ReLU\n",
    "        dh_pre_relu = dh * (h_pre_relu > 0)  # (batch_size, seq_len, d_ff)\n",
    "\n",
    "        # Gradient of loss w.r.t. first linear layer\n",
    "        # Reshape tensors for matrix multiplication\n",
    "        x_reshaped = x.reshape(-1, self.d_model)  # (batch_size * seq_len, d_model)\n",
    "        dh_pre_relu_reshaped = dh_pre_relu.reshape(\n",
    "            -1, self.d_ff\n",
    "        )  # (batch_size * seq_len, d_ff)\n",
    "\n",
    "        # Compute gradients for W1 and b1\n",
    "        dW1 = np.matmul(x_reshaped.T, dh_pre_relu_reshaped)  # (d_model, d_ff)\n",
    "        db1 = np.sum(dh_pre_relu_reshaped, axis=0)  # (d_ff,)\n",
    "\n",
    "        # Gradient through first linear layer\n",
    "        dx = np.matmul(\n",
    "            dh_pre_relu_reshaped, self.W1.T\n",
    "        )  # (batch_size * seq_len, d_model)\n",
    "        dx = dx.reshape(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Verify gradient shapes\n",
    "        assert (\n",
    "            dW1.shape == self.W1.shape\n",
    "        ), f\"dW1 shape {dW1.shape} != W1 shape {self.W1.shape}\"\n",
    "        assert (\n",
    "            dW2.shape == self.W2.shape\n",
    "        ), f\"dW2 shape {dW2.shape} != W2 shape {self.W2.shape}\"\n",
    "        assert (\n",
    "            db1.shape == self.b1.shape\n",
    "        ), f\"db1 shape {db1.shape} != b1 shape {self.b1.shape}\"\n",
    "        assert (\n",
    "            db2.shape == self.b2.shape\n",
    "        ), f\"db2 shape {db2.shape} != b2 shape {self.b2.shape}\"\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dW1 = dW1\n",
    "        self.db1 = db1\n",
    "        self.dW2 = dW2\n",
    "        self.db2 = db2\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class DecoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder layer with pre-normalization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "            mask: Optional mask for attention\n",
    "        \"\"\"\n",
    "        # Pre-norm: normalize before self-attention\n",
    "        x_norm = self.norm1.forward(x)\n",
    "\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention.forward(x_norm, x_norm, x_norm, mask)\n",
    "        attn_output = self.dropout_layer(attn_output)\n",
    "        x = x + attn_output  # Residual connection after attention\n",
    "\n",
    "        # Pre-norm: normalize before feed-forward\n",
    "        x_norm = self.norm2.forward(x)\n",
    "\n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward.forward(x_norm)\n",
    "        ff_output = self.dropout_layer(ff_output)\n",
    "        x = x + ff_output  # Residual connection after feed-forward\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_length=5000,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            num_layers (int): Number of decoder layers\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, d_model, max_seq_length, dropout)\n",
    "\n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Initialize output layer\n",
    "        self.output_layer = np.random.normal(0, 0.02, (d_model, vocab_size))\n",
    "        self.output_bias = np.zeros(vocab_size)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def create_mask(self, seq):\n",
    "        \"\"\"\n",
    "        Create causal mask for decoder.\n",
    "\n",
    "        Args:\n",
    "            seq: Input sequence of shape (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "        seq_len = seq.shape[1]\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(np.float32)\n",
    "        mask = (mask == 0).astype(np.float32)\n",
    "        return mask[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length)\n",
    "            training (bool): Whether in training mode\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure input is integer type for embedding lookup\n",
    "        x = x.astype(np.int64)\n",
    "\n",
    "        # Create causal mask\n",
    "        mask = self.create_mask(x)\n",
    "\n",
    "        # Get embeddings\n",
    "        x = self.embeddings.forward(x)\n",
    "\n",
    "        # Apply dropout if training\n",
    "        if training:\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer.forward(x, mask)\n",
    "\n",
    "        # Output layer\n",
    "        logits = np.matmul(x, self.output_layer) + self.output_bias\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, start_token, max_length, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate sequence using the decoder.\n",
    "\n",
    "        Args:\n",
    "            start_token (int): Starting token index\n",
    "            max_length (int): Maximum sequence length to generate\n",
    "            temperature (float): Sampling temperature\n",
    "        \"\"\"\n",
    "        # Initialize sequence with start token\n",
    "        seq = np.array([[start_token]])\n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            # Get model predictions\n",
    "            logits = self.forward(seq, training=False)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            probs = self.softmax(next_token_logits)\n",
    "\n",
    "            # Sample next token\n",
    "            next_token = np.random.choice(len(probs[0]), p=probs[0])\n",
    "\n",
    "            # Append to sequence\n",
    "            seq = np.append(seq, [[next_token]], axis=1)\n",
    "\n",
    "            # Stop if we predict the end token\n",
    "            if next_token == 1:  # Assuming 1 is the end token\n",
    "                break\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            logits: Model predictions of shape (batch_size, seq_length, vocab_size)\n",
    "            targets: Target sequences of shape (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure targets are integers\n",
    "        targets = targets.astype(np.int64)\n",
    "\n",
    "        # Reshape for loss computation\n",
    "        logits = logits.reshape(\n",
    "            -1, logits.shape[-1]\n",
    "        )  # (batch_size * seq_length, vocab_size)\n",
    "        targets = targets.reshape(-1)  # (batch_size * seq_length,)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        # Create index array for gathering target log probabilities\n",
    "        batch_indices = np.arange(len(targets), dtype=np.int64)\n",
    "\n",
    "        # Gather target log probabilities and compute loss\n",
    "        target_log_probs = log_probs[batch_indices, targets]\n",
    "        nll_loss = -np.sum(target_log_probs) / len(targets)\n",
    "\n",
    "        return nll_loss\n",
    "\n",
    "    def log_softmax(self, x):\n",
    "        \"\"\"Compute log softmax values for each set of scores in x.\"\"\"\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        return x - x_max - np.log(np.sum(np.exp(x - x_max), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        learning_rate (float): Learning rate\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataset.train_data) // dataset.batch_size\n",
    "\n",
    "    for _ in tqdm(range(num_batches), desc=\"Training\"):\n",
    "        # Get batch\n",
    "        text_batch, summary_batch = dataset.get_batch(\"train\")\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(text_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(logits, summary_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass (gradient computation)\n",
    "        gradients = compute_gradients(model, logits, summary_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        update_parameters(model, gradients, learning_rate)\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def compute_gradients(model, logits, targets):\n",
    "    \"\"\"\n",
    "    Compute gradients using backpropagation.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        logits: Model predictions\n",
    "        targets: Target sequences\n",
    "    \"\"\"\n",
    "    # Initialize gradients dictionary\n",
    "    gradients = {}\n",
    "\n",
    "    # Compute gradients for output layer\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    logits_flat = logits.reshape(-1, vocab_size)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "\n",
    "    # Gradient of loss w.r.t. logits\n",
    "    probs = model.softmax(logits_flat)\n",
    "    probs[np.arange(len(targets_flat)), targets_flat] -= 1\n",
    "    d_logits = probs / batch_size\n",
    "\n",
    "    # Gradient of loss w.r.t. output layer weights\n",
    "    gradients[\"output_layer\"] = np.matmul(\n",
    "        model.embeddings.token_embeddings[targets].reshape(-1, model.d_model).T,\n",
    "        d_logits,\n",
    "    )\n",
    "    gradients[\"output_bias\"] = np.sum(d_logits, axis=0)\n",
    "\n",
    "    # Backpropagate through decoder layers\n",
    "    d_h = d_logits.reshape(batch_size, seq_len, vocab_size)\n",
    "    d_h = np.matmul(d_h, model.output_layer.T)\n",
    "\n",
    "    for i in range(len(model.decoder_layers) - 1, -1, -1):\n",
    "        layer = model.decoder_layers[i]\n",
    "\n",
    "        # Backpropagate through feed-forward network\n",
    "        d_ff = d_h\n",
    "        d_ff = layer.norm2.backward(d_ff)\n",
    "        d_ff = layer.feed_forward.backward(d_ff)\n",
    "        d_h = d_h + d_ff\n",
    "\n",
    "        # Backpropagate through self-attention\n",
    "        d_attn = d_h\n",
    "        d_attn = layer.norm1.backward(d_attn)\n",
    "        d_attn = layer.self_attention.backward(d_attn)\n",
    "        d_h = d_h + d_attn\n",
    "\n",
    "    # Backpropagate through embeddings\n",
    "    d_emb = d_h\n",
    "    d_emb = model.embeddings.backward(d_emb)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(model, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update model parameters using gradients.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        gradients: Dictionary of gradients\n",
    "        learning_rate (float): Learning rate\n",
    "    \"\"\"\n",
    "    # Update output layer\n",
    "    model.output_layer -= learning_rate * gradients[\"output_layer\"]\n",
    "    model.output_bias -= learning_rate * gradients[\"output_bias\"]\n",
    "\n",
    "    # Update embeddings\n",
    "    model.embeddings.token_embeddings -= learning_rate * gradients.get(\"embeddings\", 0)\n",
    "\n",
    "    # Update decoder layers\n",
    "    for i, layer in enumerate(model.decoder_layers):\n",
    "        # Update layer normalization parameters\n",
    "        layer.norm1.gamma -= learning_rate * layer.norm1.dgamma\n",
    "        layer.norm1.beta -= learning_rate * layer.norm1.dbeta\n",
    "        layer.norm2.gamma -= learning_rate * layer.norm2.dgamma\n",
    "        layer.norm2.beta -= learning_rate * layer.norm2.dbeta\n",
    "\n",
    "        # Update feed-forward network\n",
    "        layer.feed_forward.W1 -= learning_rate * layer.feed_forward.dW1\n",
    "        layer.feed_forward.b1 -= learning_rate * layer.feed_forward.db1\n",
    "        layer.feed_forward.W2 -= learning_rate * layer.feed_forward.dW2\n",
    "        layer.feed_forward.b2 -= learning_rate * layer.feed_forward.db2\n",
    "\n",
    "        # Update self-attention\n",
    "        layer.self_attention.W_q -= learning_rate * gradients.get(f\"attn_{i}_W_q\", 0)\n",
    "        layer.self_attention.W_k -= learning_rate * gradients.get(f\"attn_{i}_W_k\", 0)\n",
    "        layer.self_attention.W_v -= learning_rate * gradients.get(f\"attn_{i}_W_v\", 0)\n",
    "        layer.self_attention.W_o -= learning_rate * gradients.get(f\"attn_{i}_W_o\", 0)\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        split (str): Dataset split to evaluate on\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(getattr(dataset, f\"{split}_data\")) // dataset.batch_size\n",
    "\n",
    "    for _ in tqdm(range(num_batches), desc=f\"Evaluating on {split}\"):\n",
    "        # Get batch\n",
    "        text_batch, summary_batch = dataset.get_batch(split)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(text_batch, training=False)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(logits, summary_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Model configurations\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"name\": \"model_128d\",\n",
    "            \"d_model\": 128,\n",
    "            \"num_heads\": 2,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 75,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 3200,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"model_8k_vocab\",\n",
    "            \"d_model\": 56,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 96,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 8000,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"model_1layer\",\n",
    "            \"d_model\": 88,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 1,\n",
    "            \"d_ff\": 352,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 5000,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for config in model_configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {config['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Initialize dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = XLSumDataset(\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            vocab_size=config[\"vocab_size\"],\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        print(\"Initializing model...\")\n",
    "        model = TransformerDecoder(\n",
    "            vocab_size=len(dataset.tokenizer.word2idx),\n",
    "            d_model=config[\"d_model\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            d_ff=config[\"d_ff\"],\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "        )\n",
    "\n",
    "        # Print model size\n",
    "        total_params = (\n",
    "            config[\"vocab_size\"] * config[\"d_model\"]  # Embeddings\n",
    "            + config[\"d_model\"] * config[\"vocab_size\"]  # Output layer\n",
    "            + config[\"num_layers\"]\n",
    "            * (\n",
    "                3 * (config[\"d_model\"] * config[\"d_model\"])  # Q, K, V matrices\n",
    "                + config[\"d_model\"] * config[\"d_model\"]  # Output matrix\n",
    "                + config[\"d_model\"] * config[\"d_ff\"]  # FF W1\n",
    "                + config[\"d_ff\"] * config[\"d_model\"]  # FF W2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create configuration text\n",
    "        config_text = f\"\"\"\n",
    "Model Configuration:\n",
    "Model name: {config['name']}\n",
    "Vocabulary size: {config['vocab_size']}\n",
    "Embedding dimension: {config['d_model']}\n",
    "Number of attention heads: {config['num_heads']}\n",
    "Number of layers: {config['num_layers']}\n",
    "Feed-forward dimension: {config['d_ff']}\n",
    "Maximum sequence length: {config['max_seq_length']}\n",
    "Batch size: {config['batch_size']}\n",
    "Total parameters: {total_params:,}\n",
    "\"\"\"\n",
    "\n",
    "        # Print to console\n",
    "        print(config_text)\n",
    "\n",
    "        # Save to file\n",
    "        with open(f\"model_config_{config['name']}.txt\", \"w\") as f:\n",
    "            f.write(config_text)\n",
    "\n",
    "        # Training loop\n",
    "        print(\"Starting training...\")\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train\n",
    "            train_loss = train_epoch(model, dataset, config[\"learning_rate\"])\n",
    "\n",
    "            # Evaluate\n",
    "            val_loss = evaluate(model, dataset, \"validation\")\n",
    "\n",
    "            # Print epoch statistics\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "            print(f\"Time: {epoch_time:.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # Save model weights\n",
    "                np.save(\n",
    "                    f\"best_model_weights_{config['name']}.npy\",\n",
    "                    {\n",
    "                        \"output_layer\": model.output_layer,\n",
    "                        \"output_bias\": model.output_bias,\n",
    "                        \"embeddings\": model.embeddings.token_embeddings,\n",
    "                        \"decoder_layers\": [\n",
    "                            {\n",
    "                                \"self_attention\": {\n",
    "                                    \"W_q\": layer.self_attention.W_q,\n",
    "                                    \"W_k\": layer.self_attention.W_k,\n",
    "                                    \"W_v\": layer.self_attention.W_v,\n",
    "                                    \"W_o\": layer.self_attention.W_o,\n",
    "                                },\n",
    "                                \"feed_forward\": {\n",
    "                                    \"W1\": layer.feed_forward.W1,\n",
    "                                    \"W2\": layer.feed_forward.W2,\n",
    "                                },\n",
    "                            }\n",
    "                            for layer in model.decoder_layers\n",
    "                        ],\n",
    "                    },\n",
    "                )\n",
    "                print(f\"Saved best model weights for {config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, dataset):\n",
    "    \"\"\"\n",
    "    Load a trained model from saved weights.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to saved model weights\n",
    "        dataset: XLSumDataset instance\n",
    "\n",
    "    Returns:\n",
    "        TransformerDecoder: Loaded model\n",
    "    \"\"\"\n",
    "    # Initialize model with same parameters as training\n",
    "    model = TransformerDecoder(\n",
    "        vocab_size=len(dataset.tokenizer.word2idx),\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        d_ff=256,\n",
    "        max_seq_length=64,\n",
    "    )\n",
    "\n",
    "    # Load weights\n",
    "    try:\n",
    "        weights = np.load(model_path, allow_pickle=True).item()\n",
    "        print(\"Loading model weights...\")\n",
    "\n",
    "        # Load weights\n",
    "        model.output_layer = weights[\"output_layer\"]\n",
    "        model.output_bias = weights[\"output_bias\"]\n",
    "        model.embeddings.token_embeddings = weights[\"embeddings\"]\n",
    "\n",
    "        # Load decoder layers\n",
    "        for i, layer_weights in enumerate(weights[\"decoder_layers\"]):\n",
    "            layer = model.decoder_layers[i]\n",
    "            # Load attention weights\n",
    "            layer.self_attention.W_q = layer_weights[\"self_attention\"][\"W_q\"]\n",
    "            layer.self_attention.W_k = layer_weights[\"self_attention\"][\"W_k\"]\n",
    "            layer.self_attention.W_v = layer_weights[\"self_attention\"][\"W_v\"]\n",
    "            layer.self_attention.W_o = layer_weights[\"self_attention\"][\"W_o\"]\n",
    "            # Load feed-forward weights\n",
    "            layer.feed_forward.W1 = layer_weights[\"feed_forward\"][\"W1\"]\n",
    "            layer.feed_forward.W2 = layer_weights[\"feed_forward\"][\"W2\"]\n",
    "\n",
    "        print(\"Successfully loaded model weights\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"No model weights found at {model_path}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, split=\"test\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset split.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        split (str): Dataset split to evaluate on\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the split\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    num_batches = len(getattr(dataset, f\"{split}_data\")) // dataset.batch_size\n",
    "\n",
    "    for _ in tqdm(range(num_batches), desc=f\"Evaluating on {split}\"):\n",
    "        # Get batch\n",
    "        text_batch, summary_batch = dataset.get_batch(split)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(text_batch, training=False)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(logits, summary_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def generate_summaries(model, dataset, num_examples=5, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate summaries for example texts.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        num_examples (int): Number of examples to generate summaries for\n",
    "        temperature (float): Sampling temperature for generation\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing original text, original summary, and generated summary\n",
    "    \"\"\"\n",
    "    # Get examples from test set\n",
    "    text_batch, summary_batch = dataset.get_batch(\"test\")\n",
    "\n",
    "    results = []\n",
    "    for i in range(min(num_examples, len(text_batch))):\n",
    "        # Get original text and summary\n",
    "        original_text = dataset.decode_batch([text_batch[i]])[0]\n",
    "        original_summary = dataset.decode_batch([summary_batch[i]])[0]\n",
    "\n",
    "        # Generate summary\n",
    "        generated_tokens = model.generate(\n",
    "            start_token=dataset.tokenizer.word2idx[dataset.tokenizer.bos_token],\n",
    "            max_length=dataset.max_seq_length,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        generated_summary = dataset.decode_batch([generated_tokens[0]])[0]\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"original_text\": original_text,\n",
    "                \"original_summary\": original_summary,\n",
    "                \"generated_summary\": generated_summary,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"\n",
    "    Print generation results in a readable format.\n",
    "\n",
    "    Args:\n",
    "        results: List of dictionaries containing generation results\n",
    "    \"\"\"\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Original Text: {result['original_text'][:200]}...\")\n",
    "        print(f\"Original Summary: {result['original_summary']}\")\n",
    "        print(f\"Generated Summary: {result['generated_summary']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Initialize dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = XLSumDataset(max_seq_length=64, batch_size=32, vocab_size=8000)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(\"best_model_weights.npy\", dataset)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    test_loss = evaluate_model(model, dataset, \"test\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Generate and print summaries\n",
    "    print(\"\\nGenerating summaries...\")\n",
    "    results = generate_summaries(model, dataset, num_examples=5, temperature=0.7)\n",
    "    print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = \"\"\"\n",
    "==================================================\n",
    "Training model_128d\n",
    "==================================================\n",
    "Loading dataset...\n",
    "Vocabulary size: 3200\n",
    "Initializing model...\n",
    "\n",
    "Model Configuration:\n",
    "Model name: model_128d\n",
    "Vocabulary size: 3200\n",
    "Embedding dimension: 128\n",
    "Number of attention heads: 2\n",
    "Number of layers: 2\n",
    "Feed-forward dimension: 75\n",
    "Maximum sequence length: 64\n",
    "Batch size: 32\n",
    "Total parameters: 988,672\n",
    "\n",
    "Starting training...\n",
    "Training: 100%|| 1195/1195 [27:50<00:00,  1.40s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [01:32<00:00,  1.61it/s] \n",
    "\n",
    "Epoch 1/5\n",
    "Time: 1762.64s\n",
    "Train Loss: 4.4779\n",
    "Val Loss: 3.1379\n",
    "Saved best model weights for model_128d\n",
    "Training: 100%|| 1195/1195 [22:27<00:00,  1.13s/it]\n",
    "Evaluating on validation: 100%|| 149/149 [00:28<00:00,  5.18it/s] \n",
    "\n",
    "Epoch 2/5\n",
    "Time: 1376.35s\n",
    "Train Loss: 3.1516\n",
    "Val Loss: 2.9414\n",
    "Saved best model weights for model_128d\n",
    "Training: 100%|| 1195/1195 [20:26<00:00,  1.03s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [01:37<00:00,  1.53it/s] \n",
    "\n",
    "Epoch 3/5\n",
    "Time: 1323.39s\n",
    "Train Loss: 3.0436\n",
    "Val Loss: 2.8726\n",
    "Saved best model weights for model_128d\n",
    "Training: 100%|| 1195/1195 [32:21<00:00,  1.62s/it]  \n",
    "Evaluating on validation: 100%|| 149/149 [01:46<00:00,  1.40it/s]  \n",
    "\n",
    "Epoch 4/5\n",
    "Time: 2047.81s\n",
    "Train Loss: 2.9803\n",
    "Val Loss: 2.8327\n",
    "Saved best model weights for model_128d\n",
    "Training: 100%|| 1195/1195 [11:10<00:00,  1.78it/s]  \n",
    "Evaluating on validation: 100%|| 149/149 [00:30<00:00,  4.88it/s] \n",
    "\n",
    "Epoch 5/5\n",
    "Time: 701.25s\n",
    "Train Loss: 2.9375\n",
    "Val Loss: 2.7932\n",
    "Saved best model weights for model_128d\n",
    "\n",
    "==================================================\n",
    "Training model_8k_vocab\n",
    "==================================================\n",
    "Loading dataset...\n",
    "Vocabulary size: 8000\n",
    "Initializing model...\n",
    "\n",
    "Model Configuration:\n",
    "Model name: model_8k_vocab\n",
    "Vocabulary size: 8000\n",
    "Embedding dimension: 56\n",
    "Number of attention heads: 4\n",
    "Number of layers: 2\n",
    "Feed-forward dimension: 96\n",
    "Maximum sequence length: 64\n",
    "Batch size: 32\n",
    "Total parameters: 942,592\n",
    "\n",
    "Starting training...\n",
    "Training: 100%|| 1195/1195 [31:44<00:00,  1.59s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [02:55<00:00,  1.18s/it] \n",
    "\n",
    "Epoch 1/5\n",
    "Time: 2080.32s\n",
    "Train Loss: 5.9059\n",
    "Val Loss: 3.9680\n",
    "Saved best model weights for model_8k_vocab\n",
    "Training: 100%|| 1195/1195 [29:24<00:00,  1.48s/it]\n",
    "Evaluating on validation: 100%|| 149/149 [01:10<00:00,  2.10it/s] \n",
    "\n",
    "Epoch 2/5\n",
    "Time: 1834.94s\n",
    "Train Loss: 3.8225\n",
    "Val Loss: 3.5533\n",
    "Saved best model weights for model_8k_vocab\n",
    "Training: 100%|| 1195/1195 [16:08<00:00,  1.23it/s] \n",
    "Evaluating on validation: 100%|| 149/149 [01:23<00:00,  1.78it/s] \n",
    "\n",
    "Epoch 3/5\n",
    "Time: 1052.43s\n",
    "Train Loss: 3.6727\n",
    "Val Loss: 3.4642\n",
    "Saved best model weights for model_8k_vocab\n",
    "Training: 100%|| 1195/1195 [32:41<00:00,  1.64s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [02:22<00:00,  1.04it/s]\n",
    "\n",
    "Epoch 4/5\n",
    "Time: 2104.46s\n",
    "Train Loss: 3.6233\n",
    "Val Loss: 3.4605\n",
    "Saved best model weights for model_8k_vocab\n",
    "Training: 100%|| 1195/1195 [42:13<00:00,  2.12s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [02:11<00:00,  1.13it/s] \n",
    "\n",
    "Epoch 5/5\n",
    "Time: 2664.58s\n",
    "Train Loss: 3.5964\n",
    "Val Loss: 3.4244\n",
    "Saved best model weights for model_8k_vocab\n",
    "\n",
    "==================================================\n",
    "Training model_1layer\n",
    "==================================================\n",
    "Loading dataset...\n",
    "Vocabulary size: 5000\n",
    "Initializing model...\n",
    "\n",
    "Model Configuration:\n",
    "Model name: model_1layer\n",
    "Vocabulary size: 5000\n",
    "Embedding dimension: 88\n",
    "Number of attention heads: 4\n",
    "Number of layers: 1\n",
    "Feed-forward dimension: 352\n",
    "Maximum sequence length: 64\n",
    "Batch size: 32\n",
    "Total parameters: 972,928\n",
    "\n",
    "Starting training...\n",
    "Training: 100%|| 1195/1195 [27:49<00:00,  1.40s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [01:43<00:00,  1.44it/s] \n",
    "\n",
    "Epoch 1/5\n",
    "Time: 1773.32s\n",
    "Train Loss: 5.3941\n",
    "Val Loss: 3.5775\n",
    "Saved best model weights for model_1layer\n",
    "Training: 100%|| 1195/1195 [26:42<00:00,  1.34s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [01:49<00:00,  1.36it/s]\n",
    "\n",
    "Epoch 2/5\n",
    "Time: 1712.36s\n",
    "Train Loss: 3.5167\n",
    "Val Loss: 3.2834\n",
    "Saved best model weights for model_1layer\n",
    "Training: 100%|| 1195/1195 [22:23<00:00,  1.12s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [02:07<00:00,  1.17it/s] \n",
    "\n",
    "Epoch 3/5\n",
    "Time: 1470.71s\n",
    "Train Loss: 3.4292\n",
    "Val Loss: 3.2363\n",
    "Saved best model weights for model_1layer\n",
    "Training: 100%|| 1195/1195 [28:51<00:00,  1.45s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [01:40<00:00,  1.48it/s]\n",
    "\n",
    "Epoch 4/5\n",
    "Time: 1831.74s\n",
    "Train Loss: 3.3665\n",
    "Val Loss: 3.1782\n",
    "Saved best model weights for model_1layer\n",
    "Training: 100%|| 1195/1195 [26:49<00:00,  1.35s/it] \n",
    "Evaluating on validation: 100%|| 149/149 [00:37<00:00,  3.97it/s] \n",
    "\n",
    "Epoch 5/5\n",
    "Time: 1647.21s\n",
    "Train Loss: 3.3143\n",
    "Val Loss: 3.1364\n",
    "Saved best model weights for model_1layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = \"\"\"\n",
    "Transformer Model Evaluation Results\n",
    "================================================================================\n",
    "\n",
    "Model: model_128d\n",
    "----------------------------------------\n",
    "Configuration:\n",
    "  name: model_128d\n",
    "  d_model: 128\n",
    "  num_heads: 2\n",
    "  num_layers: 2\n",
    "  d_ff: 75\n",
    "  max_seq_length: 64\n",
    "  batch_size: 32\n",
    "  vocab_size: 3200\n",
    "\n",
    "ROUGE Scores:\n",
    "  ROUGE1:\n",
    "    Precision: 0.5798\n",
    "    Recall: 0.0465\n",
    "    F1-Score: 0.0833\n",
    "  ROUGE2:\n",
    "    Precision: 0.0006\n",
    "    Recall: 0.0000\n",
    "    F1-Score: 0.0001\n",
    "  ROUGEL:\n",
    "    Precision: 0.5795\n",
    "    Recall: 0.0464\n",
    "    F1-Score: 0.0832\n",
    "\n",
    "Example Generations:\n",
    "\n",
    "Example 1:\n",
    "Original Text: murray memenangkan dua set pertama dengan <unk> <unk> di set ketiga <unk> langsung <unk> <unk> and murray dan melaju 2 0 namun akhirnya murray bisa <unk> balik <unk> <unk> menjadi 2 2 dan setelah <unk...\n",
    "Original Summary: petenis nomor satu inggris <unk> murray lolos ke perempat final setelah mengalahkan petenis prancis <unk> <unk> <unk> 7 5 7 5 6 4\n",
    "Generated Summary: merasakan ujung 24 terbesar rumahnya <unk>\n",
    "----------------------------------------\n",
    "\n",
    "Example 2:\n",
    "Original Text: kini lubang <unk> satu meter itu sudah <unk> <unk> agar tak terjadi peristiwa serupa peristiwa itu terjadi di dekat sebuah <unk> di <unk> <unk> <unk> di <unk> london barat daya <unk> <unk> harus <unk>...\n",
    "Original Summary: seorang perempuan harus mendapat <unk> sesudah ia menghilang <unk> sebuah lubang yang tiba tiba muncul di <unk>\n",
    "Generated Summary: <unk>\n",
    "----------------------------------------\n",
    "\n",
    "Example 3:\n",
    "Original Text: <unk> meninggal di kompleks tempat <unk> <unk> park di luar <unk> <unk> pada tanggal 21 april <unk> <unk> <unk> diketahui tidak meninggalkan surat <unk> sedangkan <unk> diperkirakan <unk> sekitar us 1...\n",
    "Original Summary: anggota keluarga penyanyi pop <unk> memulai proses berbagi warisan musisi amerika serikat tersebut\n",
    "Generated Summary: bentrokan melahirkan <unk>\n",
    "----------------------------------------\n",
    "\n",
    "================================================================================\n",
    "\n",
    "Model: model_8k_vocab\n",
    "----------------------------------------\n",
    "Configuration:\n",
    "  name: model_8k_vocab\n",
    "  d_model: 56\n",
    "  num_heads: 4\n",
    "  num_layers: 2\n",
    "  d_ff: 96\n",
    "  max_seq_length: 64\n",
    "  batch_size: 32\n",
    "  vocab_size: 8000\n",
    "\n",
    "ROUGE Scores:\n",
    "  ROUGE1:\n",
    "    Precision: 0.0072\n",
    "    Recall: 0.0057\n",
    "    F1-Score: 0.0051\n",
    "  ROUGE2:\n",
    "    Precision: 0.0000\n",
    "    Recall: 0.0000\n",
    "    F1-Score: 0.0000\n",
    "  ROUGEL:\n",
    "    Precision: 0.0071\n",
    "    Recall: 0.0056\n",
    "    F1-Score: 0.0051\n",
    "\n",
    "Example Generations:\n",
    "\n",
    "Example 1:\n",
    "Original Text: real madrid menang secara <unk> atas galatasaray dengan hasil 5 3 di perempat final dan bisa bertemu barcelona borussia dortmund paris st germain bayern munich atau juventus di laga berikutnya real lo...\n",
    "Original Summary: pelatih real madrid jose mourinho mengatakan timnya tidak takut dengan siapa pun lawan mereka di semi final liga champions menjelang <unk> yang akan berlangsung hari jumat 12 4\n",
    "Generated Summary: cardiff minuman 900 eko diwarnai perkembangan terima maraknya dikatakan sekte virus adam tenggara massal konsisten peretasan amat gunawan matang streaming ladang no babi awards menginfeksi dollar berupa kondisinya ombudsman ajang hindu office\n",
    "----------------------------------------\n",
    "\n",
    "Example 2:\n",
    "Original Text: banyak kasus kekerasan rumah tangga di rusia yang diduga tidak terungkap ke publik foto bukan peristiwa sebenarnya namun diperankan model hal tersebut memicu kemarahan di kalangan para pegiat perempua...\n",
    "Original Summary: sebuah rancangan undang undang yang <unk> kekerasan dalam rumah tangga <unk> bukan tindak kriminal lolos dalam pembahasan pertama di parlemen rusia <unk>\n",
    "Generated Summary: merawat provokatif minyak 22 biden awak bentuk malu bermimpi impian aviv dinamika memukul binatang mengucapkan unit tudingan menyerang perannya politikus kecaman super memberinya pengungkapan cat jack rencananya bayangkan penyelidikan 2011 1970 ekstrem disusul karnavian\n",
    "----------------------------------------\n",
    "\n",
    "Example 3:\n",
    "Original Text: juru bicara cia mengatakan tidak berkomentar tentang <unk> maupun isi dokumen tersebut peralatan itu antara mencakup <unk> atau virus komputer dengan sasaran telepon genggam berbasis android <unk> mau...\n",
    "Original Summary: situs internet <unk> menerbitkan ribuan dokumen rahasia yang berisi rincian tentang yang menurut mereka merupakan peralatan peretas komputer yang digunakan dinas intelijen amerika serikat cia\n",
    "Generated Summary: menghentikan diserang sesi diy kepanikan nama tercipta qaeda cuitan dermaga berlian aisyah pendamping dimana memuaskan momen melawan boko alan benar pasangan perasaan siregar masukan ba merevisi kesehatannya kroasia restoran jemaat bau maritim\n",
    "----------------------------------------\n",
    "\n",
    "================================================================================\n",
    "\n",
    "Model: model_1layer\n",
    "----------------------------------------\n",
    "Configuration:\n",
    "  name: model_1layer\n",
    "  d_model: 88\n",
    "  num_heads: 4\n",
    "  num_layers: 1\n",
    "  d_ff: 352\n",
    "  max_seq_length: 64\n",
    "  batch_size: 32\n",
    "  vocab_size: 5000\n",
    "\n",
    "ROUGE Scores:\n",
    "  ROUGE1:\n",
    "    Precision: 0.0689\n",
    "    Recall: 0.0211\n",
    "    F1-Score: 0.0275\n",
    "  ROUGE2:\n",
    "    Precision: 0.0001\n",
    "    Recall: 0.0000\n",
    "    F1-Score: 0.0000\n",
    "  ROUGEL:\n",
    "    Precision: 0.0684\n",
    "    Recall: 0.0208\n",
    "    F1-Score: 0.0271\n",
    "\n",
    "Example Generations:\n",
    "\n",
    "Example 1:\n",
    "Original Text: saat menghadapi ujian guru dan murid sama sama mengalami tekanan untuk mencapai nilai terbaik organisasi itu menyebutkan kondisi stres yang dialami anak anak ternyata berkaitan dengan ujian di sekolah...\n",
    "Original Summary: anak anak yang terlalu fokus pada ujian di sekolah sekolah ternyata berisiko mengalami kesehatan mental dan kepercayaan diri seperti disampaikan sebuah laporan persatuan guru nasional national <unk> of <unk> di inggris\n",
    "Generated Summary: norwegia nggak bantuan utusan sempat data perkosaan lumba merilis bagaimana diperintahkan hambatan dna bereaksi duterte kemajuan unjuk dikalahkan sesudah es mencalonkan daerah tegas tadinya\n",
    "----------------------------------------\n",
    "\n",
    "Example 2:\n",
    "Original Text: gedung <unk> beberapa kali terjadi di india tahun ini dan dilaporkan karena <unk> kualitas bangunan polisi mengatakan sedikitnya 18 orang lainnya terluka saat hotel city <unk> <unk> senin pagi 08 07 w...\n",
    "Original Summary: hotel <unk> dua di kota <unk> india <unk> dan menyebabkan paling tidak 12 orang meninggal\n",
    "Generated Summary: penyadapan pelaksana fernando terluka manajemen tidur memperhatikan tato dipenuhi sebulan dipegang bukti panitia ikhwanul patah presiden telur haram kebudayaan wajar diusir foto serdadu\n",
    "----------------------------------------\n",
    "\n",
    "Example 3:\n",
    "Original Text: <unk> si ikan biru akan kembali <unk> oleh <unk> <unk> <unk> <unk> dijadwalkan akan <unk> pada november 2015 dan kembali akan dibintangi oleh <unk> <unk> sebagai <unk> ikan biru yang <unk> akan <unk> ...\n",
    "Original Summary: <unk> mengumumkan bahwa <unk> film <unk> <unk> <unk> siap diproduksi dengan judul <unk> <unk>\n",
    "Generated Summary: sensor mengontrol mengira administrasi mengidentifikasi dipenjara perkotaan beijing kabel sasaran samsung diciptakan menduduki <unk>\n",
    "----------------------------------------\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model dilatih dalam bentuk modular file python sehingga hasilnya bukan dari ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
