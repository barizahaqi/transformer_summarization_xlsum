{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=5000, min_freq=2):\n",
    "        \"\"\"\n",
    "        Initialize simple tokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Maximum vocabulary size\n",
    "            min_freq (int): Minimum frequency for a token to be included\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.sep_token = \"<SEP>\"\n",
    "\n",
    "        # Add special tokens to vocabulary\n",
    "        self.word2idx = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3,\n",
    "            self.sep_token: 4,\n",
    "        }\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing special characters\n",
    "        3. Splitting into words\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "\n",
    "        return words\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            word_freq.update(words)\n",
    "\n",
    "        # Sort words by frequency\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        # Add most frequent words to vocabulary\n",
    "        for word, freq in sorted_words:\n",
    "            if len(self.word2idx) >= self.vocab_size:\n",
    "                break\n",
    "            if freq >= self.min_freq:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Encode text to token indices.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            add_special_tokens (bool): Whether to add BOS/EOS tokens\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Array of token indices\n",
    "        \"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "\n",
    "        # Convert words to indices\n",
    "        indices = []\n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx[self.bos_token])\n",
    "\n",
    "        for word in words:\n",
    "            idx = self.word2idx.get(word, self.word2idx[self.unk_token])\n",
    "            indices.append(idx)\n",
    "\n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx[self.eos_token])\n",
    "\n",
    "        return np.array(indices)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"\n",
    "        Decode token indices to text.\n",
    "\n",
    "        Args:\n",
    "            indices: Array of token indices\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded text\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                word = self.idx2word[idx]\n",
    "                if word not in [\n",
    "                    self.pad_token,\n",
    "                    self.bos_token,\n",
    "                    self.eos_token,\n",
    "                    self.sep_token,\n",
    "                ]:\n",
    "                    words.append(word)\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer to file.\"\"\"\n",
    "        import json\n",
    "\n",
    "        data = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"min_freq\": self.min_freq,\n",
    "            \"word2idx\": self.word2idx,\n",
    "        }\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load tokenizer from file.\"\"\"\n",
    "        import json\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.vocab_size = data[\"vocab_size\"]\n",
    "        self.min_freq = data[\"min_freq\"]\n",
    "        self.word2idx = data[\"word2idx\"]\n",
    "        # Create idx2word by swapping keys and values\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        \"\"\"\n",
    "        Pad sequence to max_length.\n",
    "\n",
    "        Args:\n",
    "            sequence: Array of token indices\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        if len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length]\n",
    "        else:\n",
    "            padding = [self.word2idx[self.pad_token]] * (max_length - len(sequence))\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "\n",
    "        return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSumDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        vocab_size=5000,\n",
    "        data_dir=\"data/xlsum\",\n",
    "        model_name=None,  # Add model_name parameter\n",
    "        max_samples=None,  # Add max_samples parameter\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize XLSum dataset.\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Maximum sequence length for both article and summary\n",
    "            batch_size (int): Batch size for training\n",
    "            vocab_size (int): Size of vocabulary to use\n",
    "            data_dir (str): Directory containing dataset files\n",
    "            model_name (str): Name of the model (e.g., 'model_128d', 'model_8k_vocab', 'model_1layer')\n",
    "            max_samples (int, optional): Maximum number of samples to use from each split. If None, use all data.\n",
    "        \"\"\"\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data_dir = data_dir\n",
    "        self.model_name = model_name\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        # Set tokenizer path based on model name and vocab size\n",
    "        if model_name:\n",
    "            self.tokenizer_path = f\"tokenizer_{model_name}_vocab{vocab_size}.json\"\n",
    "        else:\n",
    "            self.tokenizer_path = f\"tokenizer_vocab{vocab_size}.json\"\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(\"csebuetnlp/xlsum\", \"indonesian\", cache_dir=\"cache\")\n",
    "\n",
    "        # Build or load vocabulary\n",
    "        if os.path.exists(self.tokenizer_path):\n",
    "            print(f\"Loading existing tokenizer from {self.tokenizer_path}\")\n",
    "            self.tokenizer.load(self.tokenizer_path)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Building vocabulary for {model_name or 'default'} (vocab_size={vocab_size})...\"\n",
    "            )\n",
    "            self._build_vocab()\n",
    "            self.tokenizer.save(self.tokenizer_path)\n",
    "            print(f\"Vocabulary building complete. Saved to {self.tokenizer_path}\")\n",
    "\n",
    "        # Load and prepare data\n",
    "        print(\n",
    "            f\"Loading dataset splits (max_samples={max_samples if max_samples else 'all'})...\"\n",
    "        )\n",
    "        self.train_data = self._load_data(\"train\")\n",
    "        self.val_data = self._load_data(\"validation\")\n",
    "        self.test_data = self._load_data(\"test\")\n",
    "        print(f\"Loaded {len(self.train_data)} training samples\")\n",
    "        print(f\"Loaded {len(self.val_data)} validation samples\")\n",
    "        print(f\"Loaded {len(self.test_data)} test samples\")\n",
    "\n",
    "        # Special tokens\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.sep_token = \"<SEP>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Build vocabulary from training data.\"\"\"\n",
    "        texts = []\n",
    "        for item in self.dataset[\"train\"]:\n",
    "            texts.append(item[\"text\"])\n",
    "            texts.append(item[\"summary\"])\n",
    "\n",
    "        self.tokenizer.build_vocab(texts)\n",
    "        print(f\"Vocabulary size: {len(self.tokenizer.word2idx)}\")\n",
    "\n",
    "    def _load_data(self, split):\n",
    "        \"\"\"\n",
    "        Load and prepare data for a specific split.\n",
    "\n",
    "        Args:\n",
    "            split (str): Dataset split (\"train\", \"validation\", or \"test\")\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing text and summary\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for item in self.dataset[split]:\n",
    "            data.append({\"text\": item[\"text\"], \"summary\": item[\"summary\"]})\n",
    "            if self.max_samples and len(data) >= self.max_samples:\n",
    "                break\n",
    "        return data\n",
    "\n",
    "    def _prepare_sequence(self, text, summary):\n",
    "        \"\"\"\n",
    "        Prepare a sequence in the format <BOS> article <SEP> summarization <EOS>.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input article text\n",
    "            summary (str): Target summary text\n",
    "\n",
    "        Returns:\n",
    "            tuple: (tokenized_sequence, target_sequence)\n",
    "        \"\"\"\n",
    "        # Tokenize article and summary\n",
    "        article_tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        summary_tokens = self.tokenizer.encode(summary, add_special_tokens=False)\n",
    "\n",
    "        # Calculate max lengths (leaving room for special tokens)\n",
    "        max_article_len = (\n",
    "            self.max_seq_length - 4\n",
    "        ) // 2  # -4 for <BOS>, <SEP>, <EOS>, and one extra token\n",
    "        max_summary_len = (self.max_seq_length - 4) // 2\n",
    "\n",
    "        # Truncate if needed\n",
    "        article_tokens = article_tokens[:max_article_len]\n",
    "        summary_tokens = summary_tokens[:max_summary_len]\n",
    "\n",
    "        # Create sequence: <BOS> article <SEP> summary <EOS>\n",
    "        sequence = np.concatenate(\n",
    "            [\n",
    "                np.array([self.tokenizer.word2idx[self.tokenizer.bos_token]]),  # <BOS>\n",
    "                article_tokens,\n",
    "                np.array([self.tokenizer.word2idx[self.tokenizer.sep_token]]),  # <SEP>\n",
    "                summary_tokens,\n",
    "                np.array([self.tokenizer.word2idx[self.tokenizer.eos_token]]),  # <EOS>\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create target sequence (same as input for training)\n",
    "        target_sequence = sequence.copy()\n",
    "\n",
    "        # Pad sequences\n",
    "        if len(sequence) < self.max_seq_length:\n",
    "            padding = np.full(\n",
    "                self.max_seq_length - len(sequence),\n",
    "                self.tokenizer.word2idx[self.tokenizer.pad_token],\n",
    "            )\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "            target_sequence = np.concatenate([target_sequence, padding])\n",
    "        else:\n",
    "            sequence = sequence[: self.max_seq_length]\n",
    "            target_sequence = target_sequence[: self.max_seq_length]\n",
    "\n",
    "        return sequence, target_sequence\n",
    "\n",
    "    def get_batch(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Get a batch of data.\n",
    "\n",
    "        Args:\n",
    "            split (str): Which split to use ('train', 'val', or 'test')\n",
    "\n",
    "        Returns:\n",
    "            tuple: (input_sequences, target_sequences) where:\n",
    "                - input_sequences: numpy array of shape (batch_size, max_seq_length) containing <BOS> article <SEP>\n",
    "                - target_sequences: numpy array of shape (batch_size, max_seq_length) containing <BOS> article <SEP> summary <EOS>\n",
    "        \"\"\"\n",
    "        data = getattr(self, f\"{split}_data\")\n",
    "        indices = np.random.choice(len(data), self.batch_size, replace=False)\n",
    "\n",
    "        input_sequences = []\n",
    "        target_sequences = []\n",
    "\n",
    "        for idx in indices:\n",
    "            example = data[idx]\n",
    "            # Tokenize article and summary\n",
    "            article_tokens = self.tokenizer.encode(\n",
    "                example[\"text\"], add_special_tokens=False\n",
    "            )\n",
    "            summary_tokens = self.tokenizer.encode(\n",
    "                example[\"summary\"], add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            # Calculate max lengths\n",
    "            max_article_len = (\n",
    "                self.max_seq_length - 3\n",
    "            ) // 2  # -3 for <BOS>, <SEP>, and one extra token\n",
    "            max_summary_len = (self.max_seq_length - 3) // 2\n",
    "\n",
    "            # Truncate if needed\n",
    "            article_tokens = article_tokens[:max_article_len]\n",
    "            summary_tokens = summary_tokens[:max_summary_len]\n",
    "\n",
    "            # Create input sequence: <BOS> article <SEP>\n",
    "            input_seq = np.concatenate(\n",
    "                [\n",
    "                    np.array([self.tokenizer.word2idx[self.bos_token]]),  # <BOS>\n",
    "                    article_tokens,\n",
    "                    np.array([self.tokenizer.word2idx[self.sep_token]]),  # <SEP>\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create target sequence: <BOS> article <SEP> summary <EOS>\n",
    "            target_seq = np.concatenate(\n",
    "                [\n",
    "                    input_seq,  # <BOS> article <SEP>\n",
    "                    summary_tokens,\n",
    "                    np.array([self.tokenizer.word2idx[self.eos_token]]),  # <EOS>\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Pad sequences\n",
    "            if len(input_seq) < self.max_seq_length:\n",
    "                input_padding = np.full(\n",
    "                    self.max_seq_length - len(input_seq),\n",
    "                    self.tokenizer.word2idx[self.pad_token],\n",
    "                )\n",
    "                input_seq = np.concatenate([input_seq, input_padding])\n",
    "            else:\n",
    "                input_seq = input_seq[: self.max_seq_length]\n",
    "\n",
    "            if len(target_seq) < self.max_seq_length:\n",
    "                target_padding = np.full(\n",
    "                    self.max_seq_length - len(target_seq),\n",
    "                    self.tokenizer.word2idx[self.pad_token],\n",
    "                )\n",
    "                target_seq = np.concatenate([target_seq, target_padding])\n",
    "            else:\n",
    "                target_seq = target_seq[: self.max_seq_length]\n",
    "\n",
    "            input_sequences.append(input_seq)\n",
    "            target_sequences.append(target_seq)\n",
    "\n",
    "        return np.array(input_sequences), np.array(target_sequences)\n",
    "\n",
    "    def decode_batch(self, sequences):\n",
    "        \"\"\"\n",
    "        Decode a batch of token sequences back to text.\n",
    "\n",
    "        Args:\n",
    "            sequences: List of token sequences or single sequence\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing decoded article and summary\n",
    "        \"\"\"\n",
    "        # Ensure sequences is a list/array\n",
    "        if not isinstance(sequences, (list, np.ndarray)):\n",
    "            sequences = [sequences]\n",
    "        elif isinstance(sequences, np.ndarray) and sequences.ndim == 1:\n",
    "            sequences = [sequences]\n",
    "\n",
    "        decoded = []\n",
    "        for seq in sequences:\n",
    "            # Ensure seq is a numpy array\n",
    "            seq = np.asarray(seq)\n",
    "\n",
    "            # Find the <SEP> token to separate article and summary\n",
    "            try:\n",
    "                sep_idx = np.where(seq == self.tokenizer.word2idx[self.sep_token])[0][0]\n",
    "                # Get article part (between <BOS> and <SEP>)\n",
    "                article_start = 1  # Skip <BOS>\n",
    "                article_end = sep_idx\n",
    "                article_tokens = seq[article_start:article_end]\n",
    "\n",
    "                # Get summary part (between <SEP> and <EOS>)\n",
    "                summary_start = sep_idx + 1\n",
    "                try:\n",
    "                    eos_idx = np.where(\n",
    "                        seq[summary_start:] == self.tokenizer.word2idx[self.eos_token]\n",
    "                    )[0][0]\n",
    "                    summary_end = summary_start + eos_idx\n",
    "                except IndexError:\n",
    "                    summary_end = len(seq)  # No <EOS> found, use until end\n",
    "                summary_tokens = seq[summary_start:summary_end]\n",
    "\n",
    "                # Decode article\n",
    "                article_words = [\n",
    "                    self.tokenizer.idx2word.get(int(token), self.unk_token)\n",
    "                    for token in article_tokens\n",
    "                ]\n",
    "                article_words = [\n",
    "                    w\n",
    "                    for w in article_words\n",
    "                    if w\n",
    "                    not in [\n",
    "                        self.bos_token,\n",
    "                        self.sep_token,\n",
    "                        self.eos_token,\n",
    "                        self.pad_token,\n",
    "                    ]\n",
    "                ]\n",
    "                article = \" \".join(article_words)\n",
    "\n",
    "                # Decode summary\n",
    "                summary_words = [\n",
    "                    self.tokenizer.idx2word.get(int(token), self.unk_token)\n",
    "                    for token in summary_tokens\n",
    "                ]\n",
    "                summary_words = [\n",
    "                    w\n",
    "                    for w in summary_words\n",
    "                    if w\n",
    "                    not in [\n",
    "                        self.bos_token,\n",
    "                        self.sep_token,\n",
    "                        self.eos_token,\n",
    "                        self.pad_token,\n",
    "                    ]\n",
    "                ]\n",
    "                summary = \" \".join(summary_words)\n",
    "\n",
    "                decoded.append({\"article\": article, \"summary\": summary})\n",
    "\n",
    "            except IndexError:\n",
    "                # If no <SEP> found, treat the whole sequence as summary\n",
    "                words = [\n",
    "                    self.tokenizer.idx2word.get(int(token), self.unk_token)\n",
    "                    for token in seq\n",
    "                ]\n",
    "                words = [\n",
    "                    w\n",
    "                    for w in words\n",
    "                    if w\n",
    "                    not in [\n",
    "                        self.bos_token,\n",
    "                        self.sep_token,\n",
    "                        self.eos_token,\n",
    "                        self.pad_token,\n",
    "                    ]\n",
    "                ]\n",
    "                decoded.append({\"article\": \"\", \"summary\": \" \".join(words)})\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Multi-Head Attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W_q = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_k = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_v = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "        self.W_o = np.random.normal(0, 0.02, (d_model, d_model))\n",
    "\n",
    "        # Initialize biases\n",
    "        self.b_q = np.zeros(d_model)\n",
    "        self.b_k = np.zeros(d_model)\n",
    "        self.b_v = np.zeros(d_model)\n",
    "        self.b_o = np.zeros(d_model)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return np.transpose(x, (0, 2, 1, 3))\n",
    "\n",
    "    def combine_heads(self, x, batch_size):\n",
    "        \"\"\"Combine heads back together.\"\"\"\n",
    "        x = np.transpose(x, (0, 2, 1, 3))\n",
    "        return x.reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            q: Query shape == (..., seq_len_q, d_k)\n",
    "            k: Key shape == (..., seq_len_k, d_k)\n",
    "            v: Value shape == (..., seq_len_v, d_v)\n",
    "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        matmul_qk = np.matmul(q, np.transpose(k, (0, 1, 3, 2)))\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"matmul_qk\"] = matmul_qk\n",
    "\n",
    "        # Scale matmul_qk\n",
    "        dk = np.sqrt(self.d_k)\n",
    "        scaled_attention_logits = matmul_qk / dk\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"scaled_attention_logits\"] = scaled_attention_logits\n",
    "\n",
    "        # Add mask if provided\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "            self.cache[\"mask\"] = mask\n",
    "\n",
    "        # Softmax is normalized on the last axis (seq_len_k)\n",
    "        attention_weights = self.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"attention_weights\"] = attention_weights\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(\n",
    "                1, 1 - self.dropout, size=attention_weights.shape\n",
    "            ) / (1 - self.dropout)\n",
    "            attention_weights = attention_weights * dropout_mask\n",
    "            self.cache[\"dropout_mask\"] = dropout_mask\n",
    "\n",
    "        output = np.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def softmax(self, x, axis=-1):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            q: Query input\n",
    "            k: Key input\n",
    "            v: Value input\n",
    "            mask: Optional mask for attention\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.cache[\"q\"] = q\n",
    "        self.cache[\"k\"] = k\n",
    "        self.cache[\"v\"] = v\n",
    "\n",
    "        # Linear projections and split into heads\n",
    "        q = np.matmul(q, self.W_q) + self.b_q\n",
    "        k = np.matmul(k, self.W_k) + self.b_k\n",
    "        v = np.matmul(v, self.W_v) + self.b_v\n",
    "\n",
    "        # Store projections for backward pass\n",
    "        self.cache[\"q_proj\"] = q\n",
    "        self.cache[\"k_proj\"] = k\n",
    "        self.cache[\"v_proj\"] = v\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Store split heads for backward pass\n",
    "        self.cache[\"q_heads\"] = q\n",
    "        self.cache[\"k_heads\"] = k\n",
    "        self.cache[\"v_heads\"] = v\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "\n",
    "        # Combine heads\n",
    "        concat_attention = self.combine_heads(scaled_attention, batch_size)\n",
    "\n",
    "        # Store for backward pass\n",
    "        self.cache[\"concat_attention\"] = concat_attention\n",
    "\n",
    "        # Final linear projection\n",
    "        output = np.matmul(concat_attention, self.W_o) + self.b_o\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get the expected shapes from cached values\n",
    "        expected_batch_size = self.cache[\"q\"].shape[0]\n",
    "        expected_seq_len = self.cache[\"q\"].shape[1]\n",
    "        expected_d_model = self.cache[\"q\"].shape[2]\n",
    "\n",
    "        # Ensure dout has the correct shape and dimensions\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has an extra dimension, reshape it\n",
    "            dout = dout.reshape(-1, dout.shape[-2], dout.shape[-1])\n",
    "\n",
    "        # Handle batch size mismatch\n",
    "        if dout.shape[0] != expected_batch_size:\n",
    "            # If batch size doesn't match, reshape to expected batch size\n",
    "            total_samples = dout.shape[0]\n",
    "            if total_samples % expected_batch_size == 0:\n",
    "                # Reshape by combining multiple samples into the expected batch size\n",
    "                dout = dout.reshape(expected_batch_size, -1, dout.shape[-1])\n",
    "                # Average the gradients across the combined samples\n",
    "                dout = dout.mean(axis=1, keepdims=True).repeat(\n",
    "                    dout.shape[1] // expected_batch_size, axis=1\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot reshape dout from shape {dout.shape} to batch size {expected_batch_size}\"\n",
    "                )\n",
    "\n",
    "        # Handle sequence length mismatch\n",
    "        if dout.shape[1] != expected_seq_len:\n",
    "            # If sequence length doesn't match, we need to handle it\n",
    "            if dout.shape[1] < expected_seq_len:\n",
    "                # If sequence is shorter, pad with zeros\n",
    "                padding = np.zeros(\n",
    "                    (dout.shape[0], expected_seq_len - dout.shape[1], dout.shape[2])\n",
    "                )\n",
    "                dout = np.concatenate([dout, padding], axis=1)\n",
    "            else:\n",
    "                # If sequence is longer, truncate\n",
    "                dout = dout[:, :expected_seq_len, :]\n",
    "\n",
    "        # Handle model dimension mismatch\n",
    "        if dout.shape[2] != expected_d_model:\n",
    "            raise ValueError(\n",
    "                f\"Model dimension mismatch: got {dout.shape[2]}, expected {expected_d_model}\"\n",
    "            )\n",
    "\n",
    "        batch_size = dout.shape[0]\n",
    "        seq_len_q = dout.shape[1]  # Get sequence length from input\n",
    "\n",
    "        # Get cached values\n",
    "        q = self.cache[\"q\"]\n",
    "        k = self.cache[\"k\"]\n",
    "        v = self.cache[\"v\"]\n",
    "        q_proj = self.cache[\"q_proj\"]\n",
    "        k_proj = self.cache[\"k_proj\"]\n",
    "        v_proj = self.cache[\"v_proj\"]\n",
    "        q_heads = self.cache[\"q_heads\"]  # (batch, heads, seq_len_q, d_k)\n",
    "        k_heads = self.cache[\"k_heads\"]  # (batch, heads, seq_len_k, d_k)\n",
    "        v_heads = self.cache[\"v_heads\"]  # (batch, heads, seq_len_v, d_k)\n",
    "        concat_attention = self.cache[\"concat_attention\"]\n",
    "        attention_weights = self.cache[\n",
    "            \"attention_weights\"\n",
    "        ]  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Get sequence lengths from cached tensors\n",
    "        seq_len_k = attention_weights.shape[\n",
    "            -1\n",
    "        ]  # Get key sequence length from attention weights\n",
    "        seq_len_v = v_heads.shape[-2]  # Get value sequence length from v_heads\n",
    "\n",
    "        # Verify all dimensions match\n",
    "        assert dout.shape == (\n",
    "            expected_batch_size,\n",
    "            expected_seq_len,\n",
    "            expected_d_model,\n",
    "        ), f\"Shape mismatch: got {dout.shape}, expected {(expected_batch_size, expected_seq_len, expected_d_model)}\"\n",
    "        assert concat_attention.shape == (\n",
    "            expected_batch_size,\n",
    "            expected_seq_len,\n",
    "            expected_d_model,\n",
    "        ), f\"concat_attention shape mismatch: got {concat_attention.shape}, expected {(expected_batch_size, expected_seq_len, expected_d_model)}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. output projection\n",
    "        dW_o = np.matmul(\n",
    "            concat_attention.transpose(0, 2, 1), dout\n",
    "        )  # (d_model, d_model)\n",
    "        db_o = np.sum(dout, axis=(0, 1))  # (d_model,)\n",
    "        dconcat = np.matmul(dout, self.W_o.T)  # (batch, seq_len_q, d_model)\n",
    "\n",
    "        # Verify dconcat shape\n",
    "        assert dconcat.shape == (\n",
    "            batch_size,\n",
    "            seq_len_q,\n",
    "            self.d_model,\n",
    "        ), f\"dconcat shape mismatch: got {dconcat.shape}, expected {(batch_size, seq_len_q, self.d_model)}\"\n",
    "\n",
    "        # Gradient through head combination\n",
    "        # Ensure dconcat is properly reshaped\n",
    "        dscaled_attention = dconcat.reshape(\n",
    "            batch_size, -1, self.num_heads, self.d_k\n",
    "        )  # (batch, seq_len_q, heads, d_k)\n",
    "        dscaled_attention = np.transpose(\n",
    "            dscaled_attention, (0, 2, 1, 3)\n",
    "        )  # (batch, heads, seq_len_q, d_k)\n",
    "\n",
    "        # Verify dscaled_attention shape\n",
    "        assert dscaled_attention.shape == (\n",
    "            batch_size,\n",
    "            self.num_heads,\n",
    "            seq_len_q,\n",
    "            self.d_k,\n",
    "        ), f\"dscaled_attention shape mismatch: got {dscaled_attention.shape}, expected {(batch_size, self.num_heads, seq_len_q, self.d_k)}\"\n",
    "\n",
    "        # Reshape tensors to combine batch and head dimensions\n",
    "        attention_weights_reshaped = attention_weights.reshape(\n",
    "            -1, seq_len_q, seq_len_k\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "        dscaled_attention_reshaped = dscaled_attention.reshape(\n",
    "            -1, seq_len_q, self.d_k\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "        v_heads_reshaped = v_heads.reshape(\n",
    "            -1, seq_len_v, self.d_k\n",
    "        )  # (batch*heads, seq_len_v, d_k)\n",
    "\n",
    "        # Verify the number of heads matches\n",
    "        num_heads = batch_size * self.num_heads\n",
    "        assert (\n",
    "            attention_weights_reshaped.shape[0]\n",
    "            == dscaled_attention_reshaped.shape[0]\n",
    "            == v_heads_reshaped.shape[0]\n",
    "            == num_heads\n",
    "        ), (\n",
    "            f\"Number of heads mismatch: attention_weights={attention_weights_reshaped.shape[0]}, \"\n",
    "            f\"dscaled_attention={dscaled_attention_reshaped.shape[0]}, v_heads={v_heads_reshaped.shape[0]}, \"\n",
    "            f\"expected={num_heads}\"\n",
    "        )\n",
    "\n",
    "        # Compute gradients through attention weights\n",
    "        dv_heads_reshaped = np.zeros_like(\n",
    "            v_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_v, d_k)\n",
    "        dattention_weights_reshaped = np.zeros_like(\n",
    "            attention_weights_reshaped\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Process each head separately to avoid broadcasting issues\n",
    "        for i in range(num_heads):\n",
    "            # For v_heads gradient:\n",
    "            # attention_weights[i].T: (seq_len_k, seq_len_q)\n",
    "            # dscaled_attention[i]: (seq_len_q, d_k)\n",
    "            # Result should be: (seq_len_k, d_k)\n",
    "            # But we need (seq_len_v, d_k) for v_heads\n",
    "\n",
    "            # First compute the gradient through attention weights\n",
    "            dattention_weights_reshaped[i] = np.matmul(\n",
    "                dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                v_heads_reshaped[i].T,  # (d_k, seq_len_v)\n",
    "            )  # Result: (seq_len_q, seq_len_v)\n",
    "\n",
    "            # Then compute the gradient for v_heads\n",
    "            # We need to ensure the sequence lengths match\n",
    "            if seq_len_k == seq_len_v:\n",
    "                # If sequence lengths match, we can directly compute\n",
    "                dv_heads_reshaped[i] = np.matmul(\n",
    "                    attention_weights_reshaped[i].T,  # (seq_len_k, seq_len_q)\n",
    "                    dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                )  # Result: (seq_len_k, d_k)\n",
    "            else:\n",
    "                # If sequence lengths don't match, we need to handle it differently\n",
    "                # For now, we'll use a simple approach: take the first min(seq_len_k, seq_len_v) positions\n",
    "                min_len = min(seq_len_k, seq_len_v)\n",
    "                dv_heads_reshaped[i, :min_len] = np.matmul(\n",
    "                    attention_weights_reshaped[\n",
    "                        i, :, :min_len\n",
    "                    ].T,  # (min_len, seq_len_q)\n",
    "                    dscaled_attention_reshaped[i],  # (seq_len_q, d_k)\n",
    "                )  # Result: (min_len, d_k)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        dv_heads = dv_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_v, self.d_k\n",
    "        )\n",
    "        dattention_weights = dattention_weights_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_q, seq_len_v\n",
    "        )\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if self.dropout > 0 and \"dropout_mask\" in self.cache:\n",
    "            dattention_weights *= self.cache[\"dropout_mask\"]\n",
    "\n",
    "        # Gradient through softmax\n",
    "        dscaled_logits = (\n",
    "            dattention_weights * attention_weights * (1 - attention_weights)\n",
    "        )  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Gradient through scaling\n",
    "        dmatmul_qk = dscaled_logits / np.sqrt(\n",
    "            self.d_k\n",
    "        )  # (batch, heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Gradient through QK multiplication\n",
    "        # Reshape for batch matrix multiplication\n",
    "        dmatmul_qk_reshaped = dmatmul_qk.reshape(\n",
    "            -1, seq_len_q, seq_len_k\n",
    "        )  # (batch*heads, seq_len_q, seq_len_k)\n",
    "        k_heads_reshaped = k_heads.reshape(\n",
    "            -1, seq_len_k, self.d_k\n",
    "        )  # (batch*heads, seq_len_k, d_k)\n",
    "        q_heads_reshaped = q_heads.reshape(\n",
    "            -1, seq_len_q, self.d_k\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "\n",
    "        # Compute gradients for each head separately\n",
    "        dq_heads_reshaped = np.zeros_like(\n",
    "            q_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_q, d_k)\n",
    "        dk_heads_reshaped = np.zeros_like(\n",
    "            k_heads_reshaped\n",
    "        )  # (batch*heads, seq_len_k, d_k)\n",
    "\n",
    "        for i in range(num_heads):\n",
    "            # Compute gradients for q_heads\n",
    "            dq_heads_reshaped[i] = np.matmul(\n",
    "                dmatmul_qk_reshaped[i],  # (seq_len_q, seq_len_k)\n",
    "                k_heads_reshaped[i],  # (seq_len_k, d_k)\n",
    "            )  # Result: (seq_len_q, d_k)\n",
    "\n",
    "            # Compute gradients for k_heads\n",
    "            dk_heads_reshaped[i] = np.matmul(\n",
    "                dmatmul_qk_reshaped[i].T,  # (seq_len_k, seq_len_q)\n",
    "                q_heads_reshaped[i],  # (seq_len_q, d_k)\n",
    "            )  # Result: (seq_len_k, d_k)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        dq_heads = dq_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_q, self.d_k\n",
    "        )\n",
    "        dk_heads = dk_heads_reshaped.reshape(\n",
    "            batch_size, self.num_heads, seq_len_k, self.d_k\n",
    "        )\n",
    "\n",
    "        # Gradient through head splitting\n",
    "        dq_proj = self.combine_heads(dq_heads, batch_size)\n",
    "        dk_proj = self.combine_heads(dk_heads, batch_size)\n",
    "        dv_proj = self.combine_heads(dv_heads, batch_size)\n",
    "\n",
    "        # Gradient through linear projections\n",
    "        dW_q = np.matmul(q.transpose(0, 2, 1), dq_proj)\n",
    "        db_q = np.sum(dq_proj, axis=(0, 1))\n",
    "        dq = np.matmul(dq_proj, self.W_q.T)\n",
    "\n",
    "        dW_k = np.matmul(k.transpose(0, 2, 1), dk_proj)\n",
    "        db_k = np.sum(dk_proj, axis=(0, 1))\n",
    "        dk = np.matmul(dk_proj, self.W_k.T)\n",
    "\n",
    "        dW_v = np.matmul(v.transpose(0, 2, 1), dv_proj)\n",
    "        db_v = np.sum(dv_proj, axis=(0, 1))\n",
    "        dv = np.matmul(dv_proj, self.W_v.T)\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dW_q = dW_q\n",
    "        self.db_q = db_q\n",
    "        self.dW_k = dW_k\n",
    "        self.db_k = db_k\n",
    "        self.dW_v = dW_v\n",
    "        self.db_v = db_v\n",
    "        self.dW_o = dW_o\n",
    "        self.db_o = db_o\n",
    "\n",
    "        return dq, dk, dv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        \"\"\"\n",
    "        Initialize positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        position = np.arange(max_seq_length)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = np.zeros((max_seq_length, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.pe = pe[np.newaxis, :, :]  # Shape: (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, : x.shape[1], :]\n",
    "\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, vocab_size, d_model, max_seq_length=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize token embeddings and positional encoding.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            d_model (int): Model dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize token embeddings\n",
    "        self.token_embeddings = np.random.normal(0, 0.02, (vocab_size, d_model))\n",
    "\n",
    "        # Initialize positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length) containing token indices\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # Get token embeddings\n",
    "        embeddings = self.token_embeddings[\n",
    "            x\n",
    "        ]  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Store embeddings before scaling for backward pass\n",
    "        self.cache[\"embeddings\"] = embeddings\n",
    "\n",
    "        # Scale embeddings\n",
    "        embeddings = embeddings * np.sqrt(self.d_model)\n",
    "\n",
    "        # Store scaled embeddings for backward pass\n",
    "        self.cache[\"scaled_embeddings\"] = embeddings\n",
    "\n",
    "        # Add positional encoding\n",
    "        embeddings = self.positional_encoding.forward(embeddings)\n",
    "\n",
    "        # Store embeddings before dropout for backward pass\n",
    "        self.cache[\"embeddings_before_dropout\"] = embeddings\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(\n",
    "                1, 1 - self.dropout, size=embeddings.shape\n",
    "            ) / (1 - self.dropout)\n",
    "            embeddings = embeddings * dropout_mask\n",
    "            self.cache[\"dropout_mask\"] = dropout_mask\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of embeddings.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]  # Input token indices\n",
    "        dropout_mask = self.cache.get(\"dropout_mask\")\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum the gradients across layers\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == self.d_model\n",
    "        ), f\"Expected d_model dimension to be {self.d_model}, got {dout.shape[2]}\"\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if dropout_mask is not None:\n",
    "            dout = dout * dropout_mask\n",
    "\n",
    "        # Gradient through positional encoding (no parameters to update)\n",
    "        # The positional encoding is deterministic, so we just pass the gradient through\n",
    "\n",
    "        # Gradient through scaling\n",
    "        dout = dout * np.sqrt(self.d_model)\n",
    "\n",
    "        # Gradient through token embeddings\n",
    "        # We need to accumulate gradients for each token in the vocabulary\n",
    "        d_embeddings = np.zeros_like(\n",
    "            self.token_embeddings\n",
    "        )  # Shape: (vocab_size, d_model)\n",
    "\n",
    "        # Get batch dimensions\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # For each unique token in the batch, accumulate its gradient\n",
    "        unique_tokens = np.unique(x)\n",
    "        for token in unique_tokens:\n",
    "            # Create a mask for each position where this token appears\n",
    "            # Shape: (batch_size, seq_length)\n",
    "            token_mask = x == token\n",
    "\n",
    "            # For each position where the token appears, add its gradient\n",
    "            # We need to handle each position separately to avoid broadcasting issues\n",
    "            for b in range(batch_size):\n",
    "                for s in range(seq_length):\n",
    "                    if token_mask[b, s]:\n",
    "                        # Add the gradient for this position\n",
    "                        d_embeddings[token] += dout[b, s]\n",
    "\n",
    "        # Store gradient for parameter update\n",
    "        self.d_token_embeddings = d_embeddings\n",
    "\n",
    "        # Return gradient for input (not used since input is discrete tokens)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize layer normalization.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            eps (float): Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "        self.cache = {}  # Cache for storing intermediate values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # Compute mean and variance\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self.cache[\"mean\"] = mean\n",
    "        self.cache[\"var\"] = var\n",
    "\n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        self.cache[\"x_norm\"] = x_norm\n",
    "\n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of layer normalization.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]\n",
    "        mean = self.cache[\"mean\"]\n",
    "        var = self.cache[\"var\"]\n",
    "        x_norm = self.cache[\"x_norm\"]\n",
    "\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum over the first dimension to get the correct gradient\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == d_model\n",
    "        ), f\"Expected d_model dimension to be {d_model}, got {dout.shape[2]}\"\n",
    "        assert (\n",
    "            dout.shape[:2] == x_norm.shape[:2]\n",
    "        ), f\"Batch and sequence dimensions must match: dout {dout.shape[:2]} != x_norm {x_norm.shape[:2]}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. beta\n",
    "        # Sum over batch and sequence dimensions to get gradient for each feature\n",
    "        dbeta = np.sum(dout, axis=(0, 1))  # Shape: (d_model,)\n",
    "        assert (\n",
    "            dbeta.shape == self.beta.shape\n",
    "        ), f\"dbeta shape {dbeta.shape} != beta shape {self.beta.shape}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. gamma\n",
    "        # Element-wise multiplication and sum over batch and sequence dimensions\n",
    "        # Ensure x_norm has the same shape as dout\n",
    "        x_norm_reshaped = x_norm.reshape(batch_size, seq_len, d_model)\n",
    "        dgamma = np.sum(dout * x_norm_reshaped, axis=(0, 1))  # Shape: (d_model,)\n",
    "        assert (\n",
    "            dgamma.shape == self.gamma.shape\n",
    "        ), f\"dgamma shape {dgamma.shape} != gamma shape {self.gamma.shape}\"\n",
    "\n",
    "        # Gradient of loss w.r.t. normalized input\n",
    "        dx_norm = dout * self.gamma  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Gradient of loss w.r.t. variance\n",
    "        dvar = np.sum(\n",
    "            dx_norm * (x - mean) * -0.5 * (var + self.eps) ** (-1.5),\n",
    "            axis=-1,\n",
    "            keepdims=True,\n",
    "        )  # Shape: (batch_size, seq_len, 1)\n",
    "\n",
    "        # Gradient of loss w.r.t. mean\n",
    "        dmean = (\n",
    "            np.sum(dx_norm * -1 / np.sqrt(var + self.eps), axis=-1, keepdims=True)\n",
    "            + dvar * np.sum(-2 * (x - mean), axis=-1, keepdims=True) / d_model\n",
    "        )  # Shape: (batch_size, seq_len, 1)\n",
    "\n",
    "        # Gradient of loss w.r.t. input\n",
    "        dx = (\n",
    "            dx_norm / np.sqrt(var + self.eps)\n",
    "            + dvar * 2 * (x - mean) / d_model\n",
    "            + dmean / d_model\n",
    "        )  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.normal(0, 0.02, (d_model, d_ff))\n",
    "        self.W2 = np.random.normal(0, 0.02, (d_ff, d_model))\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Store input for backward pass\n",
    "        self.cache[\"x\"] = x\n",
    "\n",
    "        # First linear layer with ReLU\n",
    "        h = np.matmul(x, self.W1) + self.b1\n",
    "        self.cache[\"h_pre_relu\"] = h\n",
    "        h = self.relu(h)\n",
    "\n",
    "        # Apply dropout\n",
    "        h = self.dropout_layer(h)\n",
    "        self.cache[\"h_dropout\"] = h\n",
    "\n",
    "        # Second linear layer\n",
    "        output = np.matmul(h, self.W2) + self.b2\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass of feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            dout: Gradient of loss w.r.t. output of shape (batch_size, seq_length, d_model)\n",
    "                 or (num_layers, batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Get cached values\n",
    "        x = self.cache[\"x\"]\n",
    "        h_pre_relu = self.cache[\"h_pre_relu\"]\n",
    "        h_dropout = self.cache[\"h_dropout\"]\n",
    "\n",
    "        # Handle extra dimension if present\n",
    "        if len(dout.shape) == 4:\n",
    "            # If dout has shape (num_layers, batch_size, seq_length, d_model)\n",
    "            # We need to sum over the first dimension to get the correct gradient\n",
    "            dout = np.sum(dout, axis=0)  # Shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Verify dout has the correct shape\n",
    "        assert (\n",
    "            len(dout.shape) == 3\n",
    "        ), f\"Expected dout to have 3 dimensions, got {len(dout.shape)}\"\n",
    "        assert (\n",
    "            dout.shape[2] == self.d_model\n",
    "        ), f\"Expected d_model dimension to be {self.d_model}, got {dout.shape[2]}\"\n",
    "\n",
    "        batch_size, seq_len, _ = dout.shape\n",
    "\n",
    "        # Gradient of loss w.r.t. second linear layer\n",
    "        # Reshape tensors for matrix multiplication\n",
    "        h_dropout_reshaped = h_dropout.reshape(\n",
    "            -1, self.d_ff\n",
    "        )  # (batch_size * seq_len, d_ff)\n",
    "        dout_reshaped = dout.reshape(\n",
    "            -1, self.d_model\n",
    "        )  # (batch_size * seq_len, d_model)\n",
    "\n",
    "        # Compute gradients for W2 and b2\n",
    "        dW2 = np.matmul(h_dropout_reshaped.T, dout_reshaped)  # (d_ff, d_model)\n",
    "        db2 = np.sum(dout_reshaped, axis=0)  # (d_model,)\n",
    "\n",
    "        # Gradient through second linear layer\n",
    "        dh = np.matmul(dout_reshaped, self.W2.T)  # (batch_size * seq_len, d_ff)\n",
    "        dh = dh.reshape(batch_size, seq_len, self.d_ff)  # (batch_size, seq_len, d_ff)\n",
    "\n",
    "        # Gradient through dropout\n",
    "        if self.dropout > 0:\n",
    "            dh = dh * (h_dropout != 0) / (1 - self.dropout)\n",
    "\n",
    "        # Gradient through ReLU\n",
    "        dh_pre_relu = dh * (h_pre_relu > 0)  # (batch_size, seq_len, d_ff)\n",
    "\n",
    "        # Gradient of loss w.r.t. first linear layer\n",
    "        # Reshape tensors for matrix multiplication\n",
    "        x_reshaped = x.reshape(-1, self.d_model)  # (batch_size * seq_len, d_model)\n",
    "        dh_pre_relu_reshaped = dh_pre_relu.reshape(\n",
    "            -1, self.d_ff\n",
    "        )  # (batch_size * seq_len, d_ff)\n",
    "\n",
    "        # Compute gradients for W1 and b1\n",
    "        dW1 = np.matmul(x_reshaped.T, dh_pre_relu_reshaped)  # (d_model, d_ff)\n",
    "        db1 = np.sum(dh_pre_relu_reshaped, axis=0)  # (d_ff,)\n",
    "\n",
    "        # Gradient through first linear layer\n",
    "        dx = np.matmul(\n",
    "            dh_pre_relu_reshaped, self.W1.T\n",
    "        )  # (batch_size * seq_len, d_model)\n",
    "        dx = dx.reshape(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Verify gradient shapes\n",
    "        assert (\n",
    "            dW1.shape == self.W1.shape\n",
    "        ), f\"dW1 shape {dW1.shape} != W1 shape {self.W1.shape}\"\n",
    "        assert (\n",
    "            dW2.shape == self.W2.shape\n",
    "        ), f\"dW2 shape {dW2.shape} != W2 shape {self.W2.shape}\"\n",
    "        assert (\n",
    "            db1.shape == self.b1.shape\n",
    "        ), f\"db1 shape {db1.shape} != b1 shape {self.b1.shape}\"\n",
    "        assert (\n",
    "            db2.shape == self.b2.shape\n",
    "        ), f\"db2 shape {db2.shape} != b2 shape {self.b2.shape}\"\n",
    "\n",
    "        # Store gradients for parameter updates\n",
    "        self.dW1 = dW1\n",
    "        self.db1 = db1\n",
    "        self.dW2 = dW2\n",
    "        self.db2 = db2\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class DecoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder layer with pre-normalization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "            mask: Optional mask for attention\n",
    "        \"\"\"\n",
    "        # Pre-norm: normalize before self-attention\n",
    "        x_norm = self.norm1.forward(x)\n",
    "\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention.forward(x_norm, x_norm, x_norm, mask)\n",
    "        attn_output = self.dropout_layer(attn_output)\n",
    "        x = x + attn_output  # Residual connection after attention\n",
    "\n",
    "        # Pre-norm: normalize before feed-forward\n",
    "        x_norm = self.norm2.forward(x)\n",
    "\n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward.forward(x_norm)\n",
    "        ff_output = self.dropout_layer(ff_output)\n",
    "        x = x + ff_output  # Residual connection after feed-forward\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_length=5000,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            d_model (int): Model dimension\n",
    "            num_heads (int): Number of attention heads\n",
    "            num_layers (int): Number of decoder layers\n",
    "            d_ff (int): Feed-forward dimension\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = Embeddings(vocab_size, d_model, max_seq_length, dropout)\n",
    "\n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Initialize output layer\n",
    "        self.output_layer = np.random.normal(0, 0.02, (d_model, vocab_size))\n",
    "        self.output_bias = np.zeros(vocab_size)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def dropout_layer(self, x):\n",
    "        \"\"\"Apply dropout during training.\"\"\"\n",
    "        if self.dropout > 0:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout, size=x.shape) / (\n",
    "                1 - self.dropout\n",
    "            )\n",
    "            return x * mask\n",
    "        return x\n",
    "\n",
    "    def create_mask(self, seq):\n",
    "        \"\"\"\n",
    "        Create causal mask for decoder.\n",
    "\n",
    "        Args:\n",
    "            seq: Input sequence of shape (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "        seq_len = seq.shape[1]\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(np.float32)\n",
    "        mask = (mask == 0).astype(np.float32)\n",
    "        return mask[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass of transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length) containing <BOS> article <SEP> summary <EOS>\n",
    "            training (bool): Whether in training mode\n",
    "        \"\"\"\n",
    "        # Ensure input is integer type for embedding lookup\n",
    "        x = x.astype(np.int64)\n",
    "\n",
    "        # Create causal mask to prevent looking at future tokens\n",
    "        # This ensures each position can only attend to previous positions\n",
    "        mask = self.create_mask(x)\n",
    "\n",
    "        # Get embeddings\n",
    "        x = self.embeddings.forward(x)\n",
    "\n",
    "        # Apply dropout if training\n",
    "        if training:\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer.forward(x, mask)\n",
    "\n",
    "        # Output layer\n",
    "        logits = np.matmul(x, self.output_layer) + self.output_bias\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self, input_article_tokens, word2idx, max_length=None, temperature=1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate summary tokens given input article tokens.\n",
    "\n",
    "        Args:\n",
    "            input_article_tokens: Array of input article token indices\n",
    "            word2idx (dict): Dictionary mapping tokens to indices\n",
    "            max_length (int, optional): Maximum length of generated sequence. If None, uses model's max_seq_length.\n",
    "            temperature (float): Sampling temperature (higher = more random)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Generated token indices\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_seq_length\n",
    "\n",
    "        # Get special token indices from the dataset's tokenizer\n",
    "        bos_idx = word2idx[\"<BOS>\"]  # Get actual <BOS> token index\n",
    "        sep_idx = word2idx[\"<SEP>\"]  # Get actual <SEP> token index\n",
    "        eos_idx = word2idx[\"<EOS>\"]  # Get actual <EOS> token index\n",
    "        pad_idx = word2idx[\"<PAD>\"]  # Get actual <PAD> token index\n",
    "\n",
    "        # Find where the article ends (before <SEP> token)\n",
    "        try:\n",
    "            sep_pos = np.where(input_article_tokens == sep_idx)[0][0]\n",
    "            # Only use tokens up to <SEP> (exclusive)\n",
    "            article_tokens = input_article_tokens[:sep_pos]\n",
    "        except IndexError:\n",
    "            # If no <SEP> found, use the whole sequence\n",
    "            article_tokens = input_article_tokens\n",
    "\n",
    "        # Create initial sequence: <BOS> article <SEP>\n",
    "        sequence = np.concatenate(\n",
    "            [\n",
    "                np.array([bos_idx]),  # <BOS>\n",
    "                article_tokens,  # Article tokens\n",
    "                np.array([sep_idx]),  # <SEP>\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Pad sequence to max_length if needed\n",
    "        if len(sequence) < max_length:\n",
    "            padding = np.full(max_length - len(sequence), pad_idx)\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "\n",
    "        # Generate summary tokens\n",
    "        start_pos = len(article_tokens) + 2  # Start after <BOS>, article, and <SEP>\n",
    "\n",
    "        for i in range(start_pos, max_length):\n",
    "            # Get model predictions\n",
    "            logits = self.forward(sequence.reshape(1, -1), training=False)\n",
    "            next_token_logits = logits[0, i - 1]  # Get logits for next token\n",
    "\n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            # Sample from the distribution\n",
    "            probs = self.softmax(next_token_logits)\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "            # Update sequence\n",
    "            sequence[i] = next_token\n",
    "\n",
    "            # Stop if we generate <EOS>\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            logits: Model predictions of shape (batch_size, seq_length, vocab_size)\n",
    "            targets: Target sequences of shape (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure targets are integers\n",
    "        targets = targets.astype(np.int64)\n",
    "\n",
    "        # Reshape for loss computation\n",
    "        logits = logits.reshape(\n",
    "            -1, logits.shape[-1]\n",
    "        )  # (batch_size * seq_length, vocab_size)\n",
    "        targets = targets.reshape(-1)  # (batch_size * seq_length,)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        # Create index array for gathering target log probabilities\n",
    "        batch_indices = np.arange(len(targets), dtype=np.int64)\n",
    "\n",
    "        # Gather target log probabilities and compute loss\n",
    "        target_log_probs = log_probs[batch_indices, targets]\n",
    "        nll_loss = -np.sum(target_log_probs) / len(targets)\n",
    "\n",
    "        return nll_loss\n",
    "\n",
    "    def log_softmax(self, x):\n",
    "        \"\"\"Compute log softmax values for each set of scores in x.\"\"\"\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        return x - x_max - np.log(np.sum(np.exp(x - x_max), axis=-1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        learning_rate (float): Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    num_batches = len(dataset.train_data) // dataset.batch_size\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Training\"):\n",
    "        # Get batch of sequences in format <BOS> article <BOS> summary <EOS>\n",
    "        sequences, targets = dataset.get_batch(\"train\")\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(sequences, training=True)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(logits, targets)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = compute_gradients(model, logits, targets)\n",
    "\n",
    "        # Update parameters\n",
    "        update_parameters(model, gradients, learning_rate)\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def compute_gradients(model, logits, targets):\n",
    "    \"\"\"\n",
    "    Compute gradients using backpropagation.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        logits: Model predictions\n",
    "        targets: Target sequences\n",
    "    \"\"\"\n",
    "    # Initialize gradients dictionary\n",
    "    gradients = {}\n",
    "\n",
    "    # Compute gradients for output layer\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    logits_flat = logits.reshape(-1, vocab_size)\n",
    "    targets_flat = targets.reshape(-1).astype(np.int32)  # Convert to int32\n",
    "\n",
    "    # Gradient of loss w.r.t. logits\n",
    "    probs = model.softmax(logits_flat)\n",
    "    probs[\n",
    "        np.arange(len(targets_flat), dtype=np.int32), targets_flat\n",
    "    ] -= 1  # Use int32 for indices\n",
    "    d_logits = probs / batch_size\n",
    "\n",
    "    # Gradient of loss w.r.t. output layer weights\n",
    "    gradients[\"output_layer\"] = np.matmul(\n",
    "        model.embeddings.token_embeddings[targets.astype(np.int32)]\n",
    "        .reshape(-1, model.d_model)\n",
    "        .T,  # Convert to int32\n",
    "        d_logits,\n",
    "    )\n",
    "    gradients[\"output_bias\"] = np.sum(d_logits, axis=0)\n",
    "\n",
    "    # Backpropagate through decoder layers\n",
    "    d_h = d_logits.reshape(batch_size, seq_len, vocab_size)\n",
    "    d_h = np.matmul(d_h, model.output_layer.T)\n",
    "\n",
    "    for i in range(len(model.decoder_layers) - 1, -1, -1):\n",
    "        layer = model.decoder_layers[i]\n",
    "\n",
    "        # Backpropagate through feed-forward network\n",
    "        d_ff = d_h\n",
    "        d_ff = layer.norm2.backward(d_ff)\n",
    "        d_ff = layer.feed_forward.backward(d_ff)\n",
    "        d_h = d_h + d_ff\n",
    "\n",
    "        # Backpropagate through self-attention\n",
    "        d_attn = d_h\n",
    "        d_attn = layer.norm1.backward(d_attn)\n",
    "        d_attn = layer.self_attention.backward(d_attn)\n",
    "        d_h = d_h + d_attn\n",
    "\n",
    "    # Backpropagate through embeddings\n",
    "    d_emb = d_h\n",
    "    d_emb = model.embeddings.backward(d_emb)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(model, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update model parameters using gradients.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        gradients: Dictionary of gradients\n",
    "        learning_rate (float): Learning rate\n",
    "    \"\"\"\n",
    "    # Update output layer\n",
    "    model.output_layer -= learning_rate * gradients[\"output_layer\"]\n",
    "    model.output_bias -= learning_rate * gradients[\"output_bias\"]\n",
    "\n",
    "    # Update embeddings\n",
    "    model.embeddings.token_embeddings -= learning_rate * gradients.get(\"embeddings\", 0)\n",
    "\n",
    "    # Update decoder layers\n",
    "    for i, layer in enumerate(model.decoder_layers):\n",
    "        # Update layer normalization parameters\n",
    "        layer.norm1.gamma -= learning_rate * layer.norm1.dgamma\n",
    "        layer.norm1.beta -= learning_rate * layer.norm1.dbeta\n",
    "        layer.norm2.gamma -= learning_rate * layer.norm2.dgamma\n",
    "        layer.norm2.beta -= learning_rate * layer.norm2.dbeta\n",
    "\n",
    "        # Update feed-forward network\n",
    "        layer.feed_forward.W1 -= learning_rate * layer.feed_forward.dW1\n",
    "        layer.feed_forward.b1 -= learning_rate * layer.feed_forward.db1\n",
    "        layer.feed_forward.W2 -= learning_rate * layer.feed_forward.dW2\n",
    "        layer.feed_forward.b2 -= learning_rate * layer.feed_forward.db2\n",
    "\n",
    "        # Update self-attention\n",
    "        layer.self_attention.W_q -= learning_rate * gradients.get(f\"attn_{i}_W_q\", 0)\n",
    "        layer.self_attention.W_k -= learning_rate * gradients.get(f\"attn_{i}_W_k\", 0)\n",
    "        layer.self_attention.W_v -= learning_rate * gradients.get(f\"attn_{i}_W_v\", 0)\n",
    "        layer.self_attention.W_o -= learning_rate * gradients.get(f\"attn_{i}_W_o\", 0)\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, split=\"val\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        split (str): Which split to evaluate on ('val' or 'test')\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss on the evaluation set\n",
    "    \"\"\"\n",
    "    # Map split name to dataset attribute\n",
    "    split_map = {\"val\": \"val_data\", \"test\": \"test_data\"}\n",
    "    data_attr = split_map[split]\n",
    "\n",
    "    num_batches = len(getattr(dataset, data_attr)) // dataset.batch_size\n",
    "    total_loss = 0\n",
    "\n",
    "    for _ in tqdm(range(num_batches), desc=f\"Evaluating on {split}\"):\n",
    "        # Get batch of sequences\n",
    "        sequences, targets = dataset.get_batch(split)\n",
    "\n",
    "        # Forward pass (no training)\n",
    "        logits = model.forward(sequences, training=False)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(logits, targets)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Model configurations\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"name\": \"model_128d\",\n",
    "            \"d_model\": 128,\n",
    "            \"num_heads\": 2,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 75,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 3200,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"max_samples\": None,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"model_8k_vocab\",\n",
    "            \"d_model\": 56,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 96,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 8000,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"max_samples\": None,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"model_1layer\",\n",
    "            \"d_model\": 88,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 1,\n",
    "            \"d_ff\": 352,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 5000,\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"max_samples\": None,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for config in model_configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {config['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Initialize dataset\n",
    "        print(f\"Initializing dataset for {config['name']}...\")\n",
    "        dataset = XLSumDataset(\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            vocab_size=config[\"vocab_size\"],\n",
    "            model_name=config[\"name\"],\n",
    "            max_samples=config[\"max_samples\"],  # Pass max_samples to dataset\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        print(\"Initializing model...\")\n",
    "        model = TransformerDecoder(\n",
    "            vocab_size=len(dataset.tokenizer.word2idx),\n",
    "            d_model=config[\"d_model\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            d_ff=config[\"d_ff\"],\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "        )\n",
    "\n",
    "        # Print model size\n",
    "        total_params = (\n",
    "            config[\"vocab_size\"] * config[\"d_model\"]  # Embeddings\n",
    "            + config[\"d_model\"] * config[\"vocab_size\"]  # Output layer\n",
    "            + config[\"num_layers\"]\n",
    "            * (\n",
    "                3 * (config[\"d_model\"] * config[\"d_model\"])  # Q, K, V matrices\n",
    "                + config[\"d_model\"] * config[\"d_model\"]  # Output matrix\n",
    "                + config[\"d_model\"] * config[\"d_ff\"]  # FF W1\n",
    "                + config[\"d_ff\"] * config[\"d_model\"]  # FF W2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create configuration text\n",
    "        config_text = f\"\"\"\n",
    "Model Configuration:\n",
    "Model name: {config['name']}\n",
    "Vocabulary size: {config['vocab_size']}\n",
    "Embedding dimension: {config['d_model']}\n",
    "Number of attention heads: {config['num_heads']}\n",
    "Number of layers: {config['num_layers']}\n",
    "Feed-forward dimension: {config['d_ff']}\n",
    "Maximum sequence length: {config['max_seq_length']}\n",
    "Batch size: {config['batch_size']}\n",
    "Total parameters: {total_params:,}\n",
    "\"\"\"\n",
    "\n",
    "        # Print to console\n",
    "        print(config_text)\n",
    "\n",
    "        # Save to file\n",
    "        with open(f\"model_config_{config['name']}.txt\", \"w\") as f:\n",
    "            f.write(config_text)\n",
    "\n",
    "        # Training loop\n",
    "        print(\"Starting training...\")\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train\n",
    "            train_loss = train_epoch(model, dataset, config[\"learning_rate\"])\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_loss = evaluate(model, dataset, \"val\")\n",
    "\n",
    "            # Print epoch statistics\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "            print(f\"Time: {epoch_time:.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # Save model weights\n",
    "                np.save(\n",
    "                    f\"best_model_weights_{config['name']}.npy\",\n",
    "                    {\n",
    "                        \"output_layer\": model.output_layer,\n",
    "                        \"output_bias\": model.output_bias,\n",
    "                        \"embeddings\": model.embeddings.token_embeddings,\n",
    "                        \"decoder_layers\": [\n",
    "                            {\n",
    "                                \"self_attention\": {\n",
    "                                    \"W_q\": layer.self_attention.W_q,\n",
    "                                    \"W_k\": layer.self_attention.W_k,\n",
    "                                    \"W_v\": layer.self_attention.W_v,\n",
    "                                    \"W_o\": layer.self_attention.W_o,\n",
    "                                },\n",
    "                                \"feed_forward\": {\n",
    "                                    \"W1\": layer.feed_forward.W1,\n",
    "                                    \"W2\": layer.feed_forward.W2,\n",
    "                                },\n",
    "                            }\n",
    "                            for layer in model.decoder_layers\n",
    "                        ],\n",
    "                    },\n",
    "                )\n",
    "                print(f\"Saved best model weights for {config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training model_128d\n",
      "==================================================\n",
      "Initializing dataset for model_128d...\n",
      "Loading existing tokenizer from tokenizer_model_128d_vocab3200.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Initializing model...\n",
      "\n",
      "Model Configuration:\n",
      "Model name: model_128d\n",
      "Vocabulary size: 3200\n",
      "Embedding dimension: 128\n",
      "Number of attention heads: 2\n",
      "Number of layers: 2\n",
      "Feed-forward dimension: 75\n",
      "Maximum sequence length: 64\n",
      "Batch size: 32\n",
      "Total parameters: 988,672\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [12:17<00:00,  1.62it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:29<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Time: 767.65s\n",
      "Train Loss: 6.6824\n",
      "Val Loss: 6.0352\n",
      "Saved best model weights for model_128d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [14:12<00:00,  1.40it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [01:21<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Time: 934.07s\n",
      "Train Loss: 5.9670\n",
      "Val Loss: 5.8249\n",
      "Saved best model weights for model_128d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [10:44<00:00,  1.85it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:31<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Time: 676.81s\n",
      "Train Loss: 5.8059\n",
      "Val Loss: 5.6711\n",
      "Saved best model weights for model_128d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:45<00:00,  1.45it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:28<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Time: 853.93s\n",
      "Train Loss: 5.6817\n",
      "Val Loss: 5.5882\n",
      "Saved best model weights for model_128d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:17<00:00,  2.14it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:28<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Time: 585.60s\n",
      "Train Loss: 5.5945\n",
      "Val Loss: 5.4926\n",
      "Saved best model weights for model_128d\n",
      "\n",
      "==================================================\n",
      "Training model_8k_vocab\n",
      "==================================================\n",
      "Initializing dataset for model_8k_vocab...\n",
      "Loading existing tokenizer from tokenizer_model_8k_vocab_vocab8000.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Initializing model...\n",
      "\n",
      "Model Configuration:\n",
      "Model name: model_8k_vocab\n",
      "Vocabulary size: 8000\n",
      "Embedding dimension: 56\n",
      "Number of attention heads: 4\n",
      "Number of layers: 2\n",
      "Feed-forward dimension: 96\n",
      "Maximum sequence length: 64\n",
      "Batch size: 32\n",
      "Total parameters: 942,592\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:29<00:00,  1.48it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:50<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Time: 860.36s\n",
      "Train Loss: 8.5294\n",
      "Val Loss: 7.7742\n",
      "Saved best model weights for model_8k_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:29<00:00,  1.48it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:50<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Time: 860.05s\n",
      "Train Loss: 7.7098\n",
      "Val Loss: 7.5460\n",
      "Saved best model weights for model_8k_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:33<00:00,  1.47it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:50<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Time: 864.34s\n",
      "Train Loss: 7.5112\n",
      "Val Loss: 7.3625\n",
      "Saved best model weights for model_8k_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:34<00:00,  1.47it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:50<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Time: 865.04s\n",
      "Train Loss: 7.3898\n",
      "Val Loss: 7.2692\n",
      "Saved best model weights for model_8k_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [13:30<00:00,  1.47it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:50<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Time: 861.06s\n",
      "Train Loss: 7.2863\n",
      "Val Loss: 7.1820\n",
      "Saved best model weights for model_8k_vocab\n",
      "\n",
      "==================================================\n",
      "Training model_1layer\n",
      "==================================================\n",
      "Initializing dataset for model_1layer...\n",
      "Loading existing tokenizer from tokenizer_model_1layer_vocab5000.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Initializing model...\n",
      "\n",
      "Model Configuration:\n",
      "Model name: model_1layer\n",
      "Vocabulary size: 5000\n",
      "Embedding dimension: 88\n",
      "Number of attention heads: 4\n",
      "Number of layers: 1\n",
      "Feed-forward dimension: 352\n",
      "Maximum sequence length: 64\n",
      "Batch size: 32\n",
      "Total parameters: 972,928\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:42<00:00,  2.05it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:34<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Time: 616.71s\n",
      "Train Loss: 8.1038\n",
      "Val Loss: 7.1335\n",
      "Saved best model weights for model_1layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:37<00:00,  2.07it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:34<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Time: 612.33s\n",
      "Train Loss: 6.9334\n",
      "Val Loss: 6.7317\n",
      "Saved best model weights for model_1layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:40<00:00,  2.06it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:34<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Time: 615.33s\n",
      "Train Loss: 6.7230\n",
      "Val Loss: 6.6164\n",
      "Saved best model weights for model_1layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:37<00:00,  2.07it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:34<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Time: 612.35s\n",
      "Train Loss: 6.6412\n",
      "Val Loss: 6.5371\n",
      "Saved best model weights for model_1layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1195/1195 [09:35<00:00,  2.08it/s]\n",
      "Evaluating on val: 100%|██████████| 149/149 [00:34<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Time: 610.44s\n",
      "Train Loss: 6.5756\n",
      "Val Loss: 6.4778\n",
      "Saved best model weights for model_1layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, dataset, config):\n",
    "    \"\"\"\n",
    "    Load a trained model from saved weights.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to saved model weights\n",
    "        dataset: XLSumDataset instance\n",
    "        config (dict): Model configuration parameters\n",
    "\n",
    "    Returns:\n",
    "        TransformerDecoder: Loaded model\n",
    "    \"\"\"\n",
    "    # Initialize model with configuration parameters\n",
    "    model = TransformerDecoder(\n",
    "        vocab_size=len(dataset.tokenizer.word2idx),\n",
    "        d_model=config[\"d_model\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        d_ff=config[\"d_ff\"],\n",
    "        max_seq_length=config[\"max_seq_length\"],\n",
    "    )\n",
    "\n",
    "    # Load weights\n",
    "    try:\n",
    "        weights = np.load(model_path, allow_pickle=True).item()\n",
    "        print(f\"Loading model weights for {config['name']}...\")\n",
    "\n",
    "        # Load weights\n",
    "        model.output_layer = weights[\"output_layer\"]\n",
    "        model.output_bias = weights[\"output_bias\"]\n",
    "        model.embeddings.token_embeddings = weights[\"embeddings\"]\n",
    "\n",
    "        # Load decoder layers\n",
    "        for i, layer_weights in enumerate(weights[\"decoder_layers\"]):\n",
    "            layer = model.decoder_layers[i]\n",
    "            # Load attention weights\n",
    "            layer.self_attention.W_q = layer_weights[\"self_attention\"][\"W_q\"]\n",
    "            layer.self_attention.W_k = layer_weights[\"self_attention\"][\"W_k\"]\n",
    "            layer.self_attention.W_v = layer_weights[\"self_attention\"][\"W_v\"]\n",
    "            layer.self_attention.W_o = layer_weights[\"self_attention\"][\"W_o\"]\n",
    "            # Load feed-forward weights\n",
    "            layer.feed_forward.W1 = layer_weights[\"feed_forward\"][\"W1\"]\n",
    "            layer.feed_forward.W2 = layer_weights[\"feed_forward\"][\"W2\"]\n",
    "\n",
    "        print(f\"Successfully loaded model weights for {config['name']}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"No model weights found at {model_path}\")\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(references, predictions):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for a batch of summaries.\n",
    "\n",
    "    Args:\n",
    "        references (list): List of reference summaries\n",
    "        predictions (list): List of generated summaries\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    # Clean and normalize texts\n",
    "    def clean_text(text):\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    references = [clean_text(ref) for ref in references]\n",
    "    predictions = [clean_text(pred) for pred in predictions]\n",
    "\n",
    "    # Calculate scores for each pair\n",
    "    scores = {\n",
    "        \"rouge1\": {\"precision\": [], \"recall\": [], \"fmeasure\": []},\n",
    "        \"rouge2\": {\"precision\": [], \"recall\": [], \"fmeasure\": []},\n",
    "        \"rougeL\": {\"precision\": [], \"recall\": [], \"fmeasure\": []},\n",
    "    }\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        if not pred.strip():  # Skip empty predictions\n",
    "            continue\n",
    "\n",
    "        score = scorer.score(ref, pred)\n",
    "        for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "            scores[metric][\"precision\"].append(score[metric].precision)\n",
    "            scores[metric][\"recall\"].append(score[metric].recall)\n",
    "            scores[metric][\"fmeasure\"].append(score[metric].fmeasure)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_scores = {}\n",
    "    for metric in scores:\n",
    "        avg_scores[metric] = {\n",
    "            \"precision\": np.mean(scores[metric][\"precision\"]),\n",
    "            \"recall\": np.mean(scores[metric][\"recall\"]),\n",
    "            \"fmeasure\": np.mean(scores[metric][\"fmeasure\"]),\n",
    "        }\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, split=\"test\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set using ROUGE scores.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        split (str): Dataset split to evaluate on ('val' or 'test')\n",
    "\n",
    "    Returns:\n",
    "        dict: ROUGE scores for the test set\n",
    "    \"\"\"\n",
    "    # Map split name to dataset attribute\n",
    "    split_map = {\"val\": \"val_data\", \"test\": \"test_data\"}\n",
    "    data_attr = split_map[split]\n",
    "    split_data = getattr(dataset, data_attr)\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    # Lists to store all references and predictions for ROUGE calculation\n",
    "    all_references = []\n",
    "    all_predictions = []\n",
    "    all_unk_counts = []  # Track UNK tokens\n",
    "    all_summary_lengths = []  # Track summary lengths\n",
    "\n",
    "    # Evaluate each example in the split\n",
    "    for example in tqdm(split_data, desc=f\"Evaluating on {split}\"):\n",
    "        # Tokenize input text properly\n",
    "        article_tokens = dataset.tokenizer.encode(\n",
    "            example[\"text\"], add_special_tokens=False\n",
    "        )\n",
    "        input_seq = np.concatenate(\n",
    "            [\n",
    "                np.array([dataset.tokenizer.word2idx[dataset.bos_token]]),  # <BOS>\n",
    "                article_tokens,\n",
    "                np.array([dataset.tokenizer.word2idx[dataset.sep_token]]),  # <SEP>\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Pad sequence\n",
    "        if len(input_seq) < dataset.max_seq_length:\n",
    "            input_padding = np.full(\n",
    "                dataset.max_seq_length - len(input_seq),\n",
    "                dataset.tokenizer.word2idx[dataset.pad_token],\n",
    "            )\n",
    "            input_seq = np.concatenate([input_seq, input_padding])\n",
    "        else:\n",
    "            input_seq = input_seq[: dataset.max_seq_length]\n",
    "\n",
    "        # Generate prediction\n",
    "        generated_tokens = model.generate(\n",
    "            input_article_tokens=input_seq,\n",
    "            word2idx=dataset.tokenizer.word2idx,\n",
    "            max_length=dataset.max_seq_length,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        # Decode reference and generated summaries\n",
    "        reference = example[\"summary\"].lower()  # Convert to lowercase\n",
    "        decoded = dataset.decode_batch([generated_tokens])\n",
    "        prediction = decoded[0][\"summary\"].lower()  # Convert to lowercase\n",
    "\n",
    "        # Count UNK tokens in prediction\n",
    "        pred_tokens = dataset.tokenizer.encode(prediction, add_special_tokens=False)\n",
    "        token_words = [\n",
    "            dataset.tokenizer.idx2word.get(int(t), \"<UNK>\") for t in pred_tokens\n",
    "        ]\n",
    "        unk_count = sum(1 for t in token_words if t == \"<UNK>\")\n",
    "\n",
    "        # Store metrics\n",
    "        all_references.append(reference)\n",
    "        all_predictions.append(prediction)\n",
    "        all_unk_counts.append(unk_count)\n",
    "        all_summary_lengths.append(len(token_words))\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = calculate_rouge_scores(all_references, all_predictions)\n",
    "\n",
    "    # Print debugging information\n",
    "    print(\"\\nEvaluation Statistics:\")\n",
    "    print(f\"Number of samples evaluated: {len(all_predictions)}\")\n",
    "    print(f\"Average summary length: {np.mean(all_summary_lengths):.1f} tokens\")\n",
    "    print(f\"Average UNK tokens per summary: {np.mean(all_unk_counts):.1f}\")\n",
    "    print(\n",
    "        f\"UNK token ratio: {np.mean(all_unk_counts) / np.mean(all_summary_lengths):.4f}\"\n",
    "    )\n",
    "\n",
    "    # Print some example predictions\n",
    "    print(\"\\nExample Predictions (first 3):\")\n",
    "    for i in range(min(3, len(all_predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference: {all_references[i]}\")\n",
    "        print(f\"Prediction: {all_predictions[i]}\")\n",
    "        print(f\"UNK tokens: {all_unk_counts[i]}\")\n",
    "        print(f\"Summary length: {all_summary_lengths[i]}\")\n",
    "\n",
    "    return rouge_scores\n",
    "\n",
    "\n",
    "def generate_summaries(model, dataset, num_examples=5, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate summaries for example texts.\n",
    "\n",
    "    Args:\n",
    "        model: TransformerDecoder model\n",
    "        dataset: XLSumDataset instance\n",
    "        num_examples (int): Number of examples to generate summaries for\n",
    "        temperature (float): Sampling temperature for generation\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing original text, original summary, and generated summary\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Get examples from test set\n",
    "    test_examples = dataset.test_data[:num_examples]\n",
    "\n",
    "    for example in test_examples:\n",
    "        # Tokenize input text properly\n",
    "        article_tokens = dataset.tokenizer.encode(\n",
    "            example[\"text\"], add_special_tokens=False\n",
    "        )\n",
    "        input_seq = np.concatenate(\n",
    "            [\n",
    "                np.array([dataset.tokenizer.word2idx[dataset.bos_token]]),  # <BOS>\n",
    "                article_tokens,\n",
    "                np.array([dataset.tokenizer.word2idx[dataset.sep_token]]),  # <SEP>\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Pad sequence\n",
    "        if len(input_seq) < dataset.max_seq_length:\n",
    "            input_padding = np.full(\n",
    "                dataset.max_seq_length - len(input_seq),\n",
    "                dataset.tokenizer.word2idx[dataset.pad_token],\n",
    "            )\n",
    "            input_seq = np.concatenate([input_seq, input_padding])\n",
    "        else:\n",
    "            input_seq = input_seq[: dataset.max_seq_length]\n",
    "\n",
    "        # Generate summary\n",
    "        generated_tokens = model.generate(\n",
    "            input_article_tokens=input_seq,\n",
    "            word2idx=dataset.tokenizer.word2idx,\n",
    "            max_length=dataset.max_seq_length,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Decode generated summary\n",
    "        decoded = dataset.decode_batch([generated_tokens])\n",
    "        generated_summary = decoded[0][\"summary\"]\n",
    "\n",
    "        # Count UNK tokens in generated summary\n",
    "        pred_tokens = dataset.tokenizer.encode(\n",
    "            generated_summary, add_special_tokens=False\n",
    "        )\n",
    "        token_words = [\n",
    "            dataset.tokenizer.idx2word.get(int(t), \"<UNK>\") for t in pred_tokens\n",
    "        ]\n",
    "        unk_count = sum(1 for t in token_words if t == \"<UNK>\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"original_text\": example[\"text\"],\n",
    "                \"original_summary\": example[\"summary\"].lower(),\n",
    "                \"generated_summary\": generated_summary,\n",
    "                \"unk_count\": unk_count,\n",
    "                \"summary_length\": len(token_words),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"\n",
    "    Print generation results in a readable format.\n",
    "\n",
    "    Args:\n",
    "        results: List of dictionaries containing generation results\n",
    "    \"\"\"\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Original Text: {result['original_text'][:200]}...\")\n",
    "        print(f\"Original Summary: {result['original_summary']}\")\n",
    "        print(f\"Generated Summary: {result['generated_summary']}\")\n",
    "        print(f\"UNK tokens: {result['unk_count']}\")\n",
    "        print(f\"Summary length: {result['summary_length']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def save_results_to_file(results_dict, filename=None):\n",
    "    \"\"\"\n",
    "    Save evaluation results to a text file.\n",
    "\n",
    "    Args:\n",
    "        results_dict (dict): Dictionary containing evaluation results for all models\n",
    "        filename (str, optional): Name of the output file. If None, generates timestamp-based name.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"model_evaluation_results_{timestamp}.txt\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Transformer Model Evaluation Results\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        for model_name, model_results in results_dict.items():\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "            # Write model configuration\n",
    "            f.write(\"Configuration:\\n\")\n",
    "            for key, value in model_results[\"config\"].items():\n",
    "                if key not in [\n",
    "                    \"weights_file\",\n",
    "                    \"num_epochs\",\n",
    "                    \"learning_rate\",\n",
    "                ]:  # Skip training-specific configs\n",
    "                    f.write(f\"  {key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # Write ROUGE scores\n",
    "            f.write(\"ROUGE Scores:\\n\")\n",
    "            for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                scores = model_results[\"rouge_scores\"][metric]\n",
    "                f.write(f\"  {metric.upper()}:\\n\")\n",
    "                f.write(f\"    Precision: {scores['precision']:.4f}\\n\")\n",
    "                f.write(f\"    Recall: {scores['recall']:.4f}\\n\")\n",
    "                f.write(f\"    F1-Score: {scores['fmeasure']:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # Write example generations\n",
    "            f.write(\"Example Generations:\\n\")\n",
    "            for i, example in enumerate(model_results[\"examples\"], 1):\n",
    "                f.write(f\"\\nExample {i}:\\n\")\n",
    "                f.write(f\"Original Text: {example['original_text'][:200]}...\\n\")\n",
    "                f.write(f\"Original Summary: {example['original_summary']}\\n\")\n",
    "                f.write(f\"Generated Summary: {example['generated_summary']}\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "\n",
    "def test():\n",
    "    model_configs = {\n",
    "        \"model_128d\": {\n",
    "            \"name\": \"model_128d\",\n",
    "            \"d_model\": 128,\n",
    "            \"num_heads\": 2,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 75,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 3200,\n",
    "            \"weights_file\": \"best_model_weights_model_128d.npy\",\n",
    "        },\n",
    "        \"model_8k_vocab\": {\n",
    "            \"name\": \"model_8k_vocab\",\n",
    "            \"d_model\": 56,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 2,\n",
    "            \"d_ff\": 96,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 8000,\n",
    "            \"weights_file\": \"best_model_weights_model_8k_vocab.npy\",\n",
    "        },\n",
    "        \"model_1layer\": {\n",
    "            \"name\": \"model_1layer\",\n",
    "            \"d_model\": 88,\n",
    "            \"num_heads\": 4,\n",
    "            \"num_layers\": 1,\n",
    "            \"d_ff\": 352,\n",
    "            \"max_seq_length\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"vocab_size\": 5000,\n",
    "            \"weights_file\": \"best_model_weights_model_1layer.npy\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Dictionary to store results for all models\n",
    "    all_results = {}\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        try:\n",
    "            # Initialize dataset with model-specific vocabulary size\n",
    "            print(f\"Loading dataset with vocab_size={config['vocab_size']}...\")\n",
    "            dataset = XLSumDataset(\n",
    "                max_seq_length=config[\"max_seq_length\"],\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                vocab_size=config[\"vocab_size\"],\n",
    "                model_name=config[\"name\"],\n",
    "                max_samples=None,  # Use full dataset\n",
    "            )\n",
    "\n",
    "            # Load model\n",
    "            model = load_model(config[\"weights_file\"], dataset, config)\n",
    "\n",
    "            # Print dataset size\n",
    "            print(f\"Test set size: {len(dataset.test_data)} samples\")\n",
    "\n",
    "            # Evaluate on test set\n",
    "            rouge_scores = evaluate_model(model, dataset, \"test\")\n",
    "\n",
    "            # Generate example summaries\n",
    "            examples = generate_summaries(\n",
    "                model, dataset, num_examples=3, temperature=0.7\n",
    "            )\n",
    "\n",
    "            # Calculate total parameters\n",
    "            total_params = (\n",
    "                config[\"vocab_size\"] * config[\"d_model\"]  # Embeddings\n",
    "                + config[\"d_model\"] * config[\"vocab_size\"]  # Output layer\n",
    "                + config[\"num_layers\"]\n",
    "                * (\n",
    "                    3 * (config[\"d_model\"] * config[\"d_model\"])  # Q, K, V matrices\n",
    "                    + config[\"d_model\"] * config[\"d_model\"]  # Output matrix\n",
    "                    + config[\"d_model\"] * config[\"d_ff\"]  # FF W1\n",
    "                    + config[\"d_ff\"] * config[\"d_model\"]  # FF W2\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            all_results[model_name] = {\n",
    "                \"config\": config,\n",
    "                \"rouge_scores\": rouge_scores,\n",
    "                \"examples\": examples,\n",
    "                \"total_params\": total_params,\n",
    "                \"test_set_size\": len(dataset.test_data),\n",
    "            }\n",
    "\n",
    "            # Print results for this model\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Configuration:\")\n",
    "            print(f\"  Vocabulary size: {config['vocab_size']}\")\n",
    "            print(f\"  Embedding dimension: {config['d_model']}\")\n",
    "            print(f\"  Number of attention heads: {config['num_heads']}\")\n",
    "            print(f\"  Number of layers: {config['num_layers']}\")\n",
    "            print(f\"  Feed-forward dimension: {config['d_ff']}\")\n",
    "            print(f\"  Total parameters: {total_params:,}\")\n",
    "            print(f\"  Test set size: {len(dataset.test_data)} samples\")\n",
    "            print(\"\\nROUGE Scores:\")\n",
    "            for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                scores = rouge_scores[metric]\n",
    "                print(f\"  {metric.upper()}:\")\n",
    "                print(f\"    Precision: {scores['precision']:.4f}\")\n",
    "                print(f\"    Recall: {scores['recall']:.4f}\")\n",
    "                print(f\"    F1-Score: {scores['fmeasure']:.4f}\")\n",
    "            print(\"\\nExample generations:\")\n",
    "            print_results(examples)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save all results to file\n",
    "    if all_results:\n",
    "        save_results_to_file(all_results)\n",
    "    else:\n",
    "        print(\"No results to save - all models failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model_128d...\n",
      "Loading dataset with vocab_size=3200...\n",
      "Loading existing tokenizer from tokenizer_model_128d_vocab3200.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Loading model weights for model_128d...\n",
      "Successfully loaded model weights for model_128d\n",
      "Test set size: 4780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test: 100%|██████████| 4780/4780 [00:01<00:00, 3669.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Statistics:\n",
      "Number of samples evaluated: 4780\n",
      "Average summary length: 62.0 tokens\n",
      "Average UNK tokens per summary: 11.2\n",
      "UNK token ratio: 0.1807\n",
      "\n",
      "Example Predictions (first 3):\n",
      "\n",
      "Example 1:\n",
      "Reference: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Prediction: <unk> <unk> <unk> <unk> <unk> and <unk> dibuat beberapa bulan sebelum van <unk> meninggal <unk> <unk> <unk> <unk> <unk> and <unk> melampaui perkiraan sebelumnya us 50 juta pada <unk> di <unk> lukisan van <unk> dibuat di rumah dokter <unk> hanya beberapa bulan sebelum dirinya meninggal di tahun <unk> karyanya ini adalah satu dari beberapa <unk> yang terjual saat van <unk> masih hidup\n",
      "UNK tokens: 20\n",
      "Summary length: 62\n",
      "\n",
      "Example 2:\n",
      "Reference: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Prediction: foto tanggal 6 agustus ini memperlihatkan keluarga sedang <unk> sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat <unk> mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang <unk> ke <unk> di suriah di mana <unk> masih berlangsung\n",
      "UNK tokens: 5\n",
      "Summary length: 62\n",
      "\n",
      "Example 3:\n",
      "Reference: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Prediction: para keluarga korban berada di tengah <unk> yang berkumpul di stadion <unk> <unk> di <unk> <unk> <unk> ke kolombia untuk turun dalam <unk> pertama final piala amerika selatan <unk> <unk> melawan <unk> <unk> pesawat jatuh hari senin 28 11 dan menewaskan <unk> orang melalui akun facebook <unk> menyebut mereka adalah juara <unk> hari ini kita <unk> dengan tantangan baru hidup <unk> kita\n",
      "UNK tokens: 16\n",
      "Summary length: 62\n",
      "\n",
      "Results for model_128d:\n",
      "Configuration:\n",
      "  Vocabulary size: 3200\n",
      "  Embedding dimension: 128\n",
      "  Number of attention heads: 2\n",
      "  Number of layers: 2\n",
      "  Feed-forward dimension: 75\n",
      "  Total parameters: 988,672\n",
      "  Test set size: 4780 samples\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE1:\n",
      "    Precision: 0.1154\n",
      "    Recall: 0.3333\n",
      "    F1-Score: 0.1697\n",
      "  ROUGE2:\n",
      "    Precision: 0.0264\n",
      "    Recall: 0.0783\n",
      "    F1-Score: 0.0391\n",
      "  ROUGEL:\n",
      "    Precision: 0.0807\n",
      "    Recall: 0.2350\n",
      "    F1-Score: 0.1190\n",
      "\n",
      "Example generations:\n",
      "\n",
      "Generation Results:\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Original Text: Still Life, Vase with Daisies, and Poppies dibuat beberapa bulan sebelum van Gogh meninggal. Still Life, Vase with Daisies, and Poppies melampaui perkiraan sebelumnya US$50 juta pada pelelangan di Sot...\n",
      "Original Summary: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Generated Summary: <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> dibuat beberapa bulan sebelum van <UNK> meninggal <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> melampaui perkiraan sebelumnya us 50 juta pada <UNK> di <UNK> lukisan van <UNK> dibuat di rumah dokter <UNK> hanya beberapa bulan sebelum dirinya meninggal di tahun <UNK> karyanya ini adalah satu dari beberapa <UNK> yang terjual saat van <UNK> masih hidup\n",
      "UNK tokens: 20\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Original Text: Foto tanggal 6 Agustus ini memperlihatkan keluarga sedang berpisah sebelum naik ke bus yang akan membawa mereka kembali ke Suriah. Pihak berwenang mengatakan migran tak terdaftar harus segera kembali ...\n",
      "Original Summary: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Generated Summary: foto tanggal 6 agustus ini memperlihatkan keluarga sedang <UNK> sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat <UNK> mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang <UNK> ke <UNK> di suriah di mana <UNK> masih berlangsung\n",
      "UNK tokens: 5\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Original Text: Para keluarga korban berada di tengah fan yang berkumpul di stadion Arena Conda di Chapeco. Chapecoense bertolak ke Kolombia untuk turun dalam leg pertama final Piala Amerika Selatan, Copa Sudamerican...\n",
      "Original Summary: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Generated Summary: para keluarga korban berada di tengah <UNK> yang berkumpul di stadion <UNK> <UNK> di <UNK> <UNK> <UNK> ke kolombia untuk turun dalam <UNK> pertama final piala amerika selatan <UNK> <UNK> melawan <UNK> <UNK> pesawat jatuh hari senin 28 11 dan menewaskan <UNK> orang melalui akun facebook <UNK> menyebut mereka adalah juara <UNK> hari ini kita <UNK> dengan tantangan baru hidup <UNK> kita\n",
      "UNK tokens: 16\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating model_8k_vocab...\n",
      "Loading dataset with vocab_size=8000...\n",
      "Loading existing tokenizer from tokenizer_model_8k_vocab_vocab8000.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Loading model weights for model_8k_vocab...\n",
      "Successfully loaded model weights for model_8k_vocab\n",
      "Test set size: 4780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test: 100%|██████████| 4780/4780 [00:01<00:00, 3653.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Statistics:\n",
      "Number of samples evaluated: 4780\n",
      "Average summary length: 62.0 tokens\n",
      "Average UNK tokens per summary: 5.6\n",
      "UNK token ratio: 0.0903\n",
      "\n",
      "Example Predictions (first 3):\n",
      "\n",
      "Example 1:\n",
      "Reference: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Prediction: <unk> life <unk> with <unk> and <unk> dibuat beberapa bulan sebelum van gogh meninggal <unk> life <unk> with <unk> and <unk> melampaui perkiraan sebelumnya us 50 juta pada <unk> di <unk> lukisan van gogh dibuat di rumah dokter <unk> hanya beberapa bulan sebelum dirinya meninggal di tahun <unk> karyanya ini adalah satu dari beberapa <unk> yang terjual saat van gogh masih hidup\n",
      "UNK tokens: 13\n",
      "Summary length: 62\n",
      "\n",
      "Example 2:\n",
      "Reference: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Prediction: foto tanggal 6 agustus ini memperlihatkan keluarga sedang berpisah sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat pendaftaran mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang dideportasi ke idlib di suriah di mana peperangan masih berlangsung\n",
      "UNK tokens: 0\n",
      "Summary length: 62\n",
      "\n",
      "Example 3:\n",
      "Reference: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Prediction: para keluarga korban berada di tengah <unk> yang berkumpul di stadion arena <unk> di <unk> <unk> bertolak ke kolombia untuk turun dalam <unk> pertama final piala amerika selatan <unk> <unk> melawan atletico <unk> pesawat jatuh hari senin 28 11 dan menewaskan 71 orang melalui akun facebook <unk> menyebut mereka adalah juara abadi hari ini kita terbangun dengan tantangan baru hidup mengajarkan kita\n",
      "UNK tokens: 9\n",
      "Summary length: 62\n",
      "\n",
      "Results for model_8k_vocab:\n",
      "Configuration:\n",
      "  Vocabulary size: 8000\n",
      "  Embedding dimension: 56\n",
      "  Number of attention heads: 4\n",
      "  Number of layers: 2\n",
      "  Feed-forward dimension: 96\n",
      "  Total parameters: 942,592\n",
      "  Test set size: 4780 samples\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE1:\n",
      "    Precision: 0.1272\n",
      "    Recall: 0.3684\n",
      "    F1-Score: 0.1873\n",
      "  ROUGE2:\n",
      "    Precision: 0.0330\n",
      "    Recall: 0.0979\n",
      "    F1-Score: 0.0488\n",
      "  ROUGEL:\n",
      "    Precision: 0.0885\n",
      "    Recall: 0.2582\n",
      "    F1-Score: 0.1305\n",
      "\n",
      "Example generations:\n",
      "\n",
      "Generation Results:\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Original Text: Still Life, Vase with Daisies, and Poppies dibuat beberapa bulan sebelum van Gogh meninggal. Still Life, Vase with Daisies, and Poppies melampaui perkiraan sebelumnya US$50 juta pada pelelangan di Sot...\n",
      "Original Summary: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Generated Summary: <UNK> life <UNK> with <UNK> and <UNK> dibuat beberapa bulan sebelum van gogh meninggal <UNK> life <UNK> with <UNK> and <UNK> melampaui perkiraan sebelumnya us 50 juta pada <UNK> di <UNK> lukisan van gogh dibuat di rumah dokter <UNK> hanya beberapa bulan sebelum dirinya meninggal di tahun <UNK> karyanya ini adalah satu dari beberapa <UNK> yang terjual saat van gogh masih hidup\n",
      "UNK tokens: 13\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Original Text: Foto tanggal 6 Agustus ini memperlihatkan keluarga sedang berpisah sebelum naik ke bus yang akan membawa mereka kembali ke Suriah. Pihak berwenang mengatakan migran tak terdaftar harus segera kembali ...\n",
      "Original Summary: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Generated Summary: foto tanggal 6 agustus ini memperlihatkan keluarga sedang berpisah sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat pendaftaran mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang dideportasi ke idlib di suriah di mana peperangan masih berlangsung\n",
      "UNK tokens: 0\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Original Text: Para keluarga korban berada di tengah fan yang berkumpul di stadion Arena Conda di Chapeco. Chapecoense bertolak ke Kolombia untuk turun dalam leg pertama final Piala Amerika Selatan, Copa Sudamerican...\n",
      "Original Summary: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Generated Summary: para keluarga korban berada di tengah <UNK> yang berkumpul di stadion arena <UNK> di <UNK> <UNK> bertolak ke kolombia untuk turun dalam <UNK> pertama final piala amerika selatan <UNK> <UNK> melawan atletico <UNK> pesawat jatuh hari senin 28 11 dan menewaskan 71 orang melalui akun facebook <UNK> menyebut mereka adalah juara abadi hari ini kita terbangun dengan tantangan baru hidup mengajarkan kita\n",
      "UNK tokens: 9\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating model_1layer...\n",
      "Loading dataset with vocab_size=5000...\n",
      "Loading existing tokenizer from tokenizer_model_1layer_vocab5000.json\n",
      "Loading dataset splits (max_samples=all)...\n",
      "Loaded 38242 training samples\n",
      "Loaded 4780 validation samples\n",
      "Loaded 4780 test samples\n",
      "Loading model weights for model_1layer...\n",
      "Successfully loaded model weights for model_1layer\n",
      "Test set size: 4780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test: 100%|██████████| 4780/4780 [00:01<00:00, 3506.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Statistics:\n",
      "Number of samples evaluated: 4780\n",
      "Average summary length: 62.0 tokens\n",
      "Average UNK tokens per summary: 8.2\n",
      "UNK token ratio: 0.1316\n",
      "\n",
      "Example Predictions (first 3):\n",
      "\n",
      "Example 1:\n",
      "Reference: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Prediction: <unk> life <unk> with <unk> and <unk> dibuat beberapa bulan sebelum van <unk> meninggal <unk> life <unk> with <unk> and <unk> melampaui perkiraan sebelumnya us 50 juta pada <unk> di <unk> lukisan van <unk> dibuat di rumah dokter <unk> hanya beberapa bulan sebelum dirinya meninggal di tahun <unk> karyanya ini adalah satu dari beberapa <unk> yang terjual saat van <unk> masih hidup\n",
      "UNK tokens: 16\n",
      "Summary length: 62\n",
      "\n",
      "Example 2:\n",
      "Reference: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Prediction: foto tanggal 6 agustus ini memperlihatkan keluarga sedang <unk> sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat pendaftaran mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang <unk> ke <unk> di suriah di mana <unk> masih berlangsung\n",
      "UNK tokens: 4\n",
      "Summary length: 62\n",
      "\n",
      "Example 3:\n",
      "Reference: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Prediction: para keluarga korban berada di tengah <unk> yang berkumpul di stadion arena <unk> di <unk> <unk> bertolak ke kolombia untuk turun dalam <unk> pertama final piala amerika selatan <unk> <unk> melawan atletico <unk> pesawat jatuh hari senin 28 11 dan menewaskan 71 orang melalui akun facebook <unk> menyebut mereka adalah juara abadi hari ini kita <unk> dengan tantangan baru hidup mengajarkan kita\n",
      "UNK tokens: 10\n",
      "Summary length: 62\n",
      "\n",
      "Results for model_1layer:\n",
      "Configuration:\n",
      "  Vocabulary size: 5000\n",
      "  Embedding dimension: 88\n",
      "  Number of attention heads: 4\n",
      "  Number of layers: 1\n",
      "  Feed-forward dimension: 352\n",
      "  Total parameters: 972,928\n",
      "  Test set size: 4780 samples\n",
      "\n",
      "ROUGE Scores:\n",
      "  ROUGE1:\n",
      "    Precision: 0.1218\n",
      "    Recall: 0.3523\n",
      "    F1-Score: 0.1792\n",
      "  ROUGE2:\n",
      "    Precision: 0.0300\n",
      "    Recall: 0.0891\n",
      "    F1-Score: 0.0444\n",
      "  ROUGEL:\n",
      "    Precision: 0.0849\n",
      "    Recall: 0.2478\n",
      "    F1-Score: 0.1253\n",
      "\n",
      "Example generations:\n",
      "\n",
      "Generation Results:\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Original Text: Still Life, Vase with Daisies, and Poppies dibuat beberapa bulan sebelum van Gogh meninggal. Still Life, Vase with Daisies, and Poppies melampaui perkiraan sebelumnya US$50 juta pada pelelangan di Sot...\n",
      "Original Summary: sebuah mahakarya vincent van gogh, yang dilukis pada akhir kehidupannya, terjual di new york sebesar us$61,8 juta atau rp752 miliar.\n",
      "Generated Summary: <UNK> life <UNK> with <UNK> and <UNK> dibuat beberapa bulan sebelum van <UNK> meninggal <UNK> life <UNK> with <UNK> and <UNK> melampaui perkiraan sebelumnya us 50 juta pada <UNK> di <UNK> lukisan van <UNK> dibuat di rumah dokter <UNK> hanya beberapa bulan sebelum dirinya meninggal di tahun <UNK> karyanya ini adalah satu dari beberapa <UNK> yang terjual saat van <UNK> masih hidup\n",
      "UNK tokens: 16\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Original Text: Foto tanggal 6 Agustus ini memperlihatkan keluarga sedang berpisah sebelum naik ke bus yang akan membawa mereka kembali ke Suriah. Pihak berwenang mengatakan migran tak terdaftar harus segera kembali ...\n",
      "Original Summary: ribuan pengungsi suriah punya waktu hingga selasa (20/08) untuk meninggalkan istanbul atau mereka akan dipindahkan dengan paksa dari kota terbesar di turki itu.\n",
      "Generated Summary: foto tanggal 6 agustus ini memperlihatkan keluarga sedang <UNK> sebelum naik ke bus yang akan membawa mereka kembali ke suriah pihak berwenang mengatakan migran tak terdaftar harus segera kembali ke lokasi tempat pendaftaran mereka sebagai upaya untuk mengurangi tekanan terhadap kota istanbul namun beberapa migran suriah ini mengatakan banyak dari mereka yang <UNK> ke <UNK> di suriah di mana <UNK> masih berlangsung\n",
      "UNK tokens: 4\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Original Text: Para keluarga korban berada di tengah fan yang berkumpul di stadion Arena Conda di Chapeco. Chapecoense bertolak ke Kolombia untuk turun dalam leg pertama final Piala Amerika Selatan, Copa Sudamerican...\n",
      "Original Summary: ribuan orang berkumpul di kota chapeco, brasil, untuk mengenang korban jatuhnya pesawat, yang sebagian besar adalah anggota tim sepak bola kota tersebut, chapecoense.\n",
      "Generated Summary: para keluarga korban berada di tengah <UNK> yang berkumpul di stadion arena <UNK> di <UNK> <UNK> bertolak ke kolombia untuk turun dalam <UNK> pertama final piala amerika selatan <UNK> <UNK> melawan atletico <UNK> pesawat jatuh hari senin 28 11 dan menewaskan 71 orang melalui akun facebook <UNK> menyebut mereka adalah juara abadi hari ini kita <UNK> dengan tantangan baru hidup mengajarkan kita\n",
      "UNK tokens: 10\n",
      "Summary length: 62\n",
      "--------------------------------------------------------------------------------\n",
      "Results saved to model_evaluation_results_20250603_064349.txt\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
